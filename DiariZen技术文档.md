# DiariZen è¯¦å°½æŠ€æœ¯æ–‡æ¡£

## ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [é¡¹ç›®æ¶æ„è¯¦è§£](#é¡¹ç›®æ¶æ„è¯¦è§£)
   - [DiariZenç›®å½•ç»“æ„](#diarizenç›®å½•ç»“æ„)
   - [pyannote-audioç›®å½•ç»“æ„](#pyannote-audioç›®å½•ç»“æ„)
   - [æ¨¡å‹æ¶æ„è¯¦è§£](#æ¨¡å‹æ¶æ„è¯¦è§£)
   - [è®­ç»ƒæµç¨‹è¯¦è§£](#è®­ç»ƒæµç¨‹è¯¦è§£)
   - [æ¨ç†æµç¨‹è¯¦è§£](#æ¨ç†æµç¨‹è¯¦è§£)
3. [ç¯å¢ƒé…ç½®ä¸å®‰è£…](#ç¯å¢ƒé…ç½®ä¸å®‰è£…)
4. [æ ¸å¿ƒæ¶æ„åŸç†](#æ ¸å¿ƒæ¶æ„åŸç†)
5. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
6. [APIè¯¦ç»†è¯´æ˜](#apiè¯¦ç»†è¯´æ˜)
7. [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
8. [æ¨¡å‹å‰ªæ](#æ¨¡å‹å‰ªæ)
9. [è¯„ä¼°ä¸åŸºå‡†æµ‹è¯•](#è¯„ä¼°ä¸åŸºå‡†æµ‹è¯•)
10. [é«˜çº§é…ç½®](#é«˜çº§é…ç½®)
11. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
12. [å¼€å‘è€…æŒ‡å—](#å¼€å‘è€…æŒ‡å—)
13. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

---

## é¡¹ç›®æ¦‚è¿°

### ä»€ä¹ˆæ˜¯DiariZenï¼Ÿ

DiariZenæ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„**è¯´è¯äººåˆ†ç¦»ï¼ˆSpeaker Diarizationï¼‰**å·¥å…·åŒ…ï¼Œå®ƒèƒ½å¤Ÿå›ç­”"è°åœ¨ä»€ä¹ˆæ—¶å€™è¯´è¯"è¿™ä¸ªé—®é¢˜ã€‚

**è¯´è¯äººåˆ†ç¦»**æ˜¯æŒ‡ä»åŒ…å«å¤šä¸ªè¯´è¯äººçš„éŸ³é¢‘å½•éŸ³ä¸­ï¼Œè‡ªåŠ¨è¯†åˆ«å‡ºï¼š
- éŸ³é¢‘ä¸­æœ‰å‡ ä¸ªä¸åŒçš„è¯´è¯äºº
- æ¯ä¸ªæ—¶é—´æ®µæ˜¯å“ªä¸ªè¯´è¯äººåœ¨è¯´è¯
- è¾“å‡ºæ—¶é—´è½´æ ‡æ³¨ï¼ˆå¦‚ï¼š0-5ç§’æ˜¯äººAï¼Œ5-8ç§’æ˜¯äººBï¼‰

### æŠ€æœ¯ç‰¹ç‚¹

#### ğŸš€ æ ¸å¿ƒä¼˜åŠ¿
- **è‡ªç›‘ç£å­¦ä¹ **ï¼šåŸºäºMicrosoft WavLMé¢„è®­ç»ƒæ¨¡å‹ï¼Œç†è§£è¯­éŸ³è¯­ä¹‰
- **ç«¯åˆ°ç«¯è®­ç»ƒ**ï¼šä»åŸå§‹éŸ³é¢‘ç›´æ¥åˆ°è¯´è¯äººæ ‡æ³¨ï¼Œæ— éœ€æ‰‹å·¥ç‰¹å¾
- **é«˜æ€§èƒ½**ï¼šåœ¨å¤šä¸ªåŸºå‡†æ•°æ®é›†ä¸Šè¶…è¶Šä¸šç•Œæ ‡å‡†Pyannote 3.1
- **æ¨¡å‹å‹ç¼©**ï¼šæ”¯æŒç»“æ„åŒ–å‰ªæï¼Œæ¨¡å‹å¤§å°å‡å°‘80-90%è€Œæ€§èƒ½å‡ ä¹ä¸å˜
- **æ˜“ç”¨æ€§**ï¼šæä¾›ç®€æ´çš„Python APIå’Œé¢„è®­ç»ƒæ¨¡å‹

#### ğŸ“Š æ€§èƒ½è¡¨ç°
| æ•°æ®é›† | Pyannote v3.1 | DiariZen-Base | DiariZen-Large | ç›¸å¯¹æ”¹è¿› |
|--------|---------------|---------------|----------------|----------|
| AMI-SDM | 22.4% | 15.8% | **14.0%** | **37.5%** |
| AISHELL-4 | 12.2% | 10.7% | **9.8%** | **19.7%** |
| AliMeeting | 24.4% | 14.1% | **12.5%** | **48.8%** |
| VoxConverse | 11.3% | 9.7% | **9.2%** | **18.6%** |

#### ğŸ—ï¸ æŠ€æœ¯æ¶æ„
```
åŸå§‹éŸ³é¢‘ â†’ WavLMç‰¹å¾æå– â†’ Conformerç¼–ç  â†’ è¯­éŸ³æ´»åŠ¨æ£€æµ‹ â†’ è¯´è¯äººåµŒå…¥ â†’ èšç±» â†’ åˆ†ç¦»ç»“æœ
```

---

## é¡¹ç›®æ¶æ„è¯¦è§£

### DiariZenç›®å½•ç»“æ„

DiariZené¡¹ç›®é‡‡ç”¨äº†æ¨¡å—åŒ–çš„è®¾è®¡ï¼Œå°†ä¸åŒåŠŸèƒ½ç»„ä»¶åˆ†ç¦»åˆ°ç‹¬ç«‹çš„ç›®å½•ä¸­ã€‚ä»¥ä¸‹æ˜¯é¡¹ç›®çš„ä¸»è¦ç›®å½•ç»“æ„ï¼š

#### æ ¹ç›®å½•ç»“æ„
```
DiariZen/
â”œâ”€â”€ diarizen/                    # æ ¸å¿ƒDiariZenä»£ç åŒ…
â”œâ”€â”€ pyannote-audio/              # pyannoteéŸ³é¢‘å¤„ç†åº“ï¼ˆå­æ¨¡å—ï¼‰
â”œâ”€â”€ recipes/                     # è®­ç»ƒé…ç½®å’Œè„šæœ¬
â”œâ”€â”€ cache/                       # æ¨¡å‹ç¼“å­˜ç›®å½•
â”œâ”€â”€ dscore/                      # è¯„ä¼°å·¥å…·
â”œâ”€â”€ example/                     # ç¤ºä¾‹æ•°æ®
â”œâ”€â”€ batch_diarize_*.py           # æ‰¹é‡å¤„ç†è„šæœ¬
â”œâ”€â”€ quick_start.py               # å¿«é€Ÿå¼€å§‹è„šæœ¬
â”œâ”€â”€ requirements.txt             # ä¾èµ–åˆ—è¡¨
â””â”€â”€ pyproject.toml               # é¡¹ç›®é…ç½®
```

#### diarizen/ æ ¸å¿ƒåŒ…ç»“æ„
```
diarizen/
â”œâ”€â”€ __init__.py                  # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ models/                      # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ eend/                    # ç«¯åˆ°ç«¯æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ model_wavlm_conformer.py    # WavLM+Conformeræ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ model_fbank_conformer.py    # FBank+Conformeræ¨¡å‹
â”‚   â”‚   â””â”€â”€ model_pyannote.py            # pyannoteå…¼å®¹æ¨¡å‹
â”‚   â”œâ”€â”€ module/                  # åŸºç¡€æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ conformer.py         # Conformerç¼–ç å™¨
â”‚   â”‚   â”œâ”€â”€ wavlm_config.py      # WavLMé…ç½®
â”‚   â”‚   â”œâ”€â”€ wav2vec2/            # wav2vec2ç›¸å…³æ¨¡å—
â”‚   â”‚   â””â”€â”€ speechbrain_feats.py # è¯­éŸ³ç‰¹å¾æå–
â”‚   â””â”€â”€ pruning/                 # æ¨¡å‹å‰ªæ
â”‚       â”œâ”€â”€ model_distill_prune.py
â”‚       â””â”€â”€ utils.py
â”œâ”€â”€ pipelines/                   # æ¨ç†ç®¡é“
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ inference.py             # æ¨ç†ç®¡é“å®ç°
â”‚   â””â”€â”€ utils.py                 # ç®¡é“å·¥å…·å‡½æ•°
â”œâ”€â”€ clustering/                  # èšç±»ç®—æ³•
â”‚   â””â”€â”€ VBx.py                   # VBxå˜åˆ†è´å¶æ–¯èšç±»
â”œâ”€â”€ trainer_*.py                 # è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ trainer_dual_opt.py      # åŒä¼˜åŒ–å™¨è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ trainer_single_opt.py    # å•ä¼˜åŒ–å™¨è®­ç»ƒå™¨
â”‚   â””â”€â”€ trainer_distill_prune.py # è’¸é¦å‰ªæè®­ç»ƒå™¨
â”œâ”€â”€ utils.py                     # é€šç”¨å·¥å…·å‡½æ•°
â”œâ”€â”€ logger.py                    # æ—¥å¿—å·¥å…·
â”œâ”€â”€ optimization.py              # ä¼˜åŒ–ç›¸å…³
â”œâ”€â”€ ckpt_utils.py                # æ£€æŸ¥ç‚¹å·¥å…·
â””â”€â”€ noam_updater.py              # Noamå­¦ä¹ ç‡è°ƒåº¦
```

#### recipes/ é…ç½®å’Œè„šæœ¬
```
recipes/
â”œâ”€â”€ diar_ssl/                    # è‡ªç›‘ç£è¯´è¯äººåˆ†ç¦»
â”‚   â”œâ”€â”€ conf/                    # é…ç½®æ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ wavlm_updated_conformer.toml    # WavLM+Conformeré…ç½®
â”‚   â”‚   â”œâ”€â”€ fbank_conformer.toml           # FBank+Conformeré…ç½®
â”‚   â”‚   â”œâ”€â”€ wavlm_frozen_conformer.toml    # å†»ç»“WavLMé…ç½®
â”‚   â”‚   â””â”€â”€ pyannote_baseline.toml         # pyannoteåŸºçº¿é…ç½®
â”‚   â”œâ”€â”€ run_dual_opt.py          # åŒä¼˜åŒ–å™¨è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ run_single_opt.py        # å•ä¼˜åŒ–å™¨è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ dataset.py               # æ•°æ®é›†å®šä¹‰
â”‚   â””â”€â”€ README.md
â””â”€â”€ diar_ssl_pruning/            # å‰ªæç‰ˆæœ¬
    â”œâ”€â”€ conf/                    # å‰ªæé…ç½®æ–‡ä»¶
    â”œâ”€â”€ run_distill_prune.py     # è’¸é¦å‰ªæè®­ç»ƒè„šæœ¬
    â”œâ”€â”€ apply_pruning.py         # åº”ç”¨å‰ªæè„šæœ¬
    â””â”€â”€ get_wavlm_from_finetuned.py
```

### pyannote-audioç›®å½•ç»“æ„

pyannote-audioæ˜¯DiariZençš„åŸºç¡€éŸ³é¢‘å¤„ç†åº“ï¼Œæä¾›äº†å®Œæ•´çš„è¯´è¯äººåˆ†ç¦»ç®¡é“ã€‚ä»¥ä¸‹æ˜¯å…¶è¯¦ç»†ç›®å½•ç»“æ„ï¼š

#### pyannote/audio/ ä¸»åŒ…ç»“æ„
```
pyannote/audio/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ core/                        # æ ¸å¿ƒç»„ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ model.py                 # åŸºç¡€æ¨¡å‹ç±»
â”‚   â”œâ”€â”€ pipeline.py              # ç®¡é“åŸºç±»
â”‚   â”œâ”€â”€ inference.py             # æ¨ç†å¼•æ“
â”‚   â”œâ”€â”€ io.py                    # è¾“å…¥è¾“å‡ºå¤„ç†
â”‚   â””â”€â”€ callback.py              # å›è°ƒæœºåˆ¶
â”œâ”€â”€ models/                      # é¢„è®­ç»ƒæ¨¡å‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ segmentation/            # åˆ†å‰²æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ PyanNet.py           # PyanNetåˆ†å‰²æ¨¡å‹
â”‚   â”‚   â””â”€â”€ SSeRiouSS.py         # SSeRiouSSåˆ†å‰²æ¨¡å‹
â”‚   â”œâ”€â”€ embedding/               # åµŒå…¥æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ wespeaker/           # WeSpeakeråµŒå…¥
â”‚   â”‚   â”‚   â”œâ”€â”€ resnet.py        # ResNetéª¨å¹²ç½‘ç»œ
â”‚   â”‚   â”‚   â”œâ”€â”€ convert.py       # æ¨¡å‹è½¬æ¢å·¥å…·
â”‚   â”‚   â”‚   â””â”€â”€ LICENSE.WeSpeaker
â”‚   â”‚   â””â”€â”€ xvector.py           # X-VectoråµŒå…¥
â”‚   â””â”€â”€ blocks/                  # åŸºç¡€æ„å»ºå—
â”‚       â”œâ”€â”€ pooling.py           # æ± åŒ–å±‚
â”‚       â””â”€â”€ sincnet.py           # SincNetå·ç§¯
â”œâ”€â”€ pipelines/                   # å¤„ç†ç®¡é“
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ speaker_diarization.py   # è¯´è¯äººåˆ†ç¦»ç®¡é“ â­â­â­
â”‚   â”œâ”€â”€ clustering.py            # èšç±»ç®—æ³•
â”‚   â”œâ”€â”€ speaker_verification.py  # è¯´è¯äººéªŒè¯
â”‚   â”œâ”€â”€ voice_activity_detection.py # VADæ£€æµ‹
â”‚   â”œâ”€â”€ multilabel.py            # å¤šæ ‡ç­¾å¤„ç†
â”‚   â”œâ”€â”€ resegmentation.py        # é‡åˆ†å‰²
â”‚   â”œâ”€â”€ overlapped_speech_detection.py # é‡å è¯­éŸ³æ£€æµ‹
â”‚   â””â”€â”€ utils/                   # ç®¡é“å·¥å…·
â”‚       â”œâ”€â”€ diarization.py       # è¯´è¯äººåˆ†ç¦»å·¥å…·
â”‚       â”œâ”€â”€ getter.py            # æ¨¡å‹è·å–å™¨
â”‚       â”œâ”€â”€ hook.py              # é’©å­å‡½æ•°
â”‚       â””â”€â”€ oracle.py            # é¢„è¨€æœºè¯„ä¼°
â”œâ”€â”€ tasks/                       # ä»»åŠ¡å®šä¹‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ segmentation/            # åˆ†å‰²ä»»åŠ¡
â”‚   â”‚   â”œâ”€â”€ speaker_diarization.py   # è¯´è¯äººåˆ†ç¦»ä»»åŠ¡
â”‚   â”‚   â”œâ”€â”€ voice_activity_detection.py # VADä»»åŠ¡
â”‚   â”‚   â”œâ”€â”€ overlapped_speech_detection.py # OSDä»»åŠ¡
â”‚   â”‚   â””â”€â”€ multilabel.py        # å¤šæ ‡ç­¾åˆ†å‰²ä»»åŠ¡
â”‚   â””â”€â”€ embedding/               # åµŒå…¥ä»»åŠ¡
â”‚       â”œâ”€â”€ arcface.py           # ArcFaceæŸå¤±
â”‚       â””â”€â”€ mixins.py            # ä»»åŠ¡æ··å…¥
â”œâ”€â”€ torchmetrics/                # PyTorchæŒ‡æ ‡
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ audio/                   # éŸ³é¢‘æŒ‡æ ‡
â”‚   â”‚   â””â”€â”€ diarization_error_rate.py # DERæŒ‡æ ‡
â”‚   â””â”€â”€ classification/          # åˆ†ç±»æŒ‡æ ‡
â”‚       â””â”€â”€ equal_error_rate.py  # EERæŒ‡æ ‡
â”œâ”€â”€ utils/                       # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loss.py                  # æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ metric.py                # è¯„ä¼°æŒ‡æ ‡
â”‚   â”œâ”€â”€ multi_task.py            # å¤šä»»åŠ¡å¤„ç†
â”‚   â”œâ”€â”€ params.py                # å‚æ•°ç®¡ç†
â”‚   â”œâ”€â”€ permutation.py           # æ’åˆ—å·¥å…·
â”‚   â”œâ”€â”€ powerset.py              # å¹‚é›†ç¼–ç 
â”‚   â”œâ”€â”€ preprocessors.py         # é¢„å¤„ç†å™¨
â”‚   â”œâ”€â”€ preview.py               # é¢„è§ˆå·¥å…·
â”‚   â”œâ”€â”€ probe.py                 # æ¢é’ˆå·¥å…·
â”‚   â”œâ”€â”€ protocol.py              # åè®®å®šä¹‰
â”‚   â”œâ”€â”€ random.py                # éšæœºå·¥å…·
â”‚   â”œâ”€â”€ receptive_field.py       # æ„Ÿå—é‡è®¡ç®—
â”‚   â”œâ”€â”€ reproducibility.py       # å¯é‡å¤æ€§
â”‚   â”œâ”€â”€ signal.py                # ä¿¡å·å¤„ç†
â”‚   â””â”€â”€ version.py               # ç‰ˆæœ¬ç®¡ç†
â”œâ”€â”€ cli/                         # å‘½ä»¤è¡Œæ¥å£
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluate.py              # è¯„ä¼°å‘½ä»¤
â”‚   â”œâ”€â”€ pretrained.py            # é¢„è®­ç»ƒæ¨¡å‹ç®¡ç†
â”‚   â””â”€â”€ train.py                 # è®­ç»ƒå‘½ä»¤
â””â”€â”€ sample/                      # ç¤ºä¾‹æ•°æ®
    â”œâ”€â”€ sample.wav
    â””â”€â”€ sample.rttm
```

#### å„å­ç³»ç»Ÿè¯¦è§£

### ğŸ¯ æ ¸å¿ƒç³»ç»Ÿ (core/)

pyannote-audioçš„æ ¸å¿ƒç³»ç»Ÿæä¾›äº†ç»Ÿä¸€çš„æ¥å£å’ŒæŠ½è±¡ï¼š

**Modelç±»** (`core/model.py`):
- **ä½œç”¨**: å®šä¹‰äº†æ‰€æœ‰éŸ³é¢‘æ¨¡å‹çš„åŸºç±»
- **å…³é”®ç‰¹æ€§**:
  - ç»Ÿä¸€çš„æ¨¡å‹æ¥å£
  - è‡ªåŠ¨å‚æ•°ç®¡ç†
  - ä»»åŠ¡è§„æ ¼å®šä¹‰
  - æ„Ÿå—é‡è®¡ç®—

**Pipelineç±»** (`core/pipeline.py`):
- **ä½œç”¨**: æä¾›å¯é…ç½®çš„å¤„ç†ç®¡é“æ¡†æ¶
- **å…³é”®ç‰¹æ€§**:
  - å‚æ•°åŒ–é…ç½®
  - è‡ªåŠ¨å‚æ•°ä¼˜åŒ–
  - æ‰¹é‡å¤„ç†æ”¯æŒ
  - é”™è¯¯å¤„ç†æœºåˆ¶

**Inferenceç±»** (`core/inference.py`):
- **ä½œç”¨**: ç»Ÿä¸€çš„æ¨¡å‹æ¨ç†å¼•æ“
- **å…³é”®ç‰¹æ€§**:
  - æ»‘åŠ¨çª—å£æ¨ç†
  - æ‰¹å¤„ç†ä¼˜åŒ–
  - è®¾å¤‡ç®¡ç†
  - å†…å­˜ä¼˜åŒ–

### ğŸ§  æ¨¡å‹ç³»ç»Ÿ (models/)

**åˆ†å‰²æ¨¡å‹** (`models/segmentation/`):
- **PyanNet**: åŸºäºTCNçš„è½»é‡çº§åˆ†å‰²æ¨¡å‹
- **SSeRiouSS**: åŸºäºResNetçš„é«˜ç²¾åº¦åˆ†å‰²æ¨¡å‹
- **å…±åŒç‰¹æ€§**:
  - å¤šæ ‡ç­¾åˆ†å‰²è¾“å‡º
  - å¹‚é›†ç¼–ç æ”¯æŒ
  - æ—¶åºå»ºæ¨¡èƒ½åŠ›

**åµŒå…¥æ¨¡å‹** (`models/embedding/`):
- **WeSpeaker ResNet**: å¤§è§„æ¨¡é¢„è®­ç»ƒåµŒå…¥æ¨¡å‹
- **X-Vector**: ä¼ ç»Ÿä½†æœ‰æ•ˆçš„åµŒå…¥æ–¹æ³•
- **å…±åŒç‰¹æ€§**:
  - è¯´è¯äººè¡¨å¾å­¦ä¹ 
  - ç›¸ä¼¼åº¦åº¦é‡
  - èšç±»å‹å¥½

### ğŸ”§ ç®¡é“ç³»ç»Ÿ (pipelines/)

**SpeakerDiarizationç®¡é“** (`pipelines/speaker_diarization.py`):
è¿™æ˜¯DiariZençš„æ ¸å¿ƒç®¡é“ï¼Œå®ç°äº†å®Œæ•´çš„è¯´è¯äººåˆ†ç¦»æµç¨‹ï¼š

```python
class SpeakerDiarization(SpeakerDiarizationMixin, Pipeline):
    """è¯´è¯äººåˆ†ç¦»ç®¡é“çš„æ ¸å¿ƒå®ç°"""
```

**ç®¡é“æµç¨‹**:
1. **è¯­éŸ³æ´»åŠ¨æ£€æµ‹**: è¯†åˆ«æœ‰å£°æ®µ
2. **è¯´è¯äººåˆ†å‰²**: å°†éŸ³é¢‘åˆ†ä¸ºè¯´è¯äººç‰‡æ®µ
3. **åµŒå…¥æå–**: ä¸ºæ¯ä¸ªç‰‡æ®µç”Ÿæˆè¯´è¯äººåµŒå…¥
4. **èšç±»**: å°†ç›¸ä¼¼åµŒå…¥å½’ç±»ä¸ºåŒä¸€è¯´è¯äºº
5. **åå¤„ç†**: ä¼˜åŒ–å’Œæ ¼å¼åŒ–ç»“æœ

**èšç±»ç®—æ³•** (`pipelines/clustering.py`):
- **AgglomerativeClustering**: å±‚æ¬¡èšç±»ï¼Œå¿«é€Ÿä¸”ç¨³å®š
- **VBxClustering**: å˜åˆ†è´å¶æ–¯èšç±»ï¼Œå‡†ç¡®ä½†è¾ƒæ…¢
- **é€‰æ‹©ç­–ç•¥**: åŸºäºéŸ³é¢‘æ—¶é•¿å’Œå‡†ç¡®æ€§éœ€æ±‚é€‰æ‹©

### ğŸ“‹ ä»»åŠ¡ç³»ç»Ÿ (tasks/)

**åˆ†å‰²ä»»åŠ¡** (`tasks/segmentation/`):
- **SpeakerDiarization**: å¤šè¯´è¯äººåœºæ™¯çš„åˆ†å‰²
- **VoiceActivityDetection**: å•è¯´è¯äººè¯­éŸ³æ£€æµ‹
- **OverlappedSpeechDetection**: é‡å è¯­éŸ³æ£€æµ‹

**åµŒå…¥ä»»åŠ¡** (`tasks/embedding/`):
- **SpeakerEmbedding**: è¯´è¯äººè¡¨å¾å­¦ä¹ 
- **ArcFace**: åˆ†ç±»å‹å¥½çš„åº¦é‡å­¦ä¹ 

### ğŸ“Š è¯„ä¼°ç³»ç»Ÿ (torchmetrics/)

**DER (Diarization Error Rate)**:
- **è®¡ç®—å…¬å¼**: DER = (FA + MISS + CONFUSION) / TOTAL_SPEECH_TIME
- **ç»„æˆéƒ¨åˆ†**:
  - FA (False Alarm): è¯¯æ£€æ—¶é—´
  - MISS: æ¼æ£€æ—¶é—´
  - CONFUSION: è¯´è¯äººæ··æ·†æ—¶é—´

---

### æ¨¡å‹æ¶æ„è¯¦è§£

#### DiariZenæ¨¡å‹æ¶æ„

DiariZençš„æ ¸å¿ƒåˆ›æ–°åœ¨äºå°†WavLMé¢„è®­ç»ƒæ¨¡å‹ä¸Conformerç¼–ç å™¨ç›¸ç»“åˆï¼Œé‡‡ç”¨ç«¯åˆ°ç«¯çš„è¯´è¯äººåˆ†ç¦»æ¶æ„ã€‚

##### æ•´ä½“æ¶æ„å›¾
```
åŸå§‹éŸ³é¢‘ (16kHz)
    â†“
WavLMç‰¹å¾æå–å™¨ (13å±‚Transformer)
    â†“
å¤šå±‚ç‰¹å¾èåˆ (åŠ æƒæ±‚å’Œ)
    â†“
çº¿æ€§æŠ•å½± (768â†’256ç»´)
    â†“
LayerNormå½’ä¸€åŒ–
    â†“
Conformerç¼–ç å™¨ (4å±‚)
    â†“
åˆ†ç±»å™¨ (256â†’å¹‚é›†ç±»åˆ«)
    â†“
å¤šæ ‡ç­¾äºŒå…ƒäº¤å‰ç†µæŸå¤±
```

##### WavLMç‰¹å¾æå–å™¨è¯¦è§£

**WavLM (Wav Large Model)** æ˜¯Microsoftå¼€å‘çš„ wav2vec 2.0 çš„å‡çº§ç‰ˆæœ¬ï¼š

```python
class WavLMFeatureExtractor:
    def __init__(self, model_path, layer_num=13):
        # åŠ è½½é¢„è®­ç»ƒWavLMæ¨¡å‹
        self.model = self.load_wavlm_model(model_path)
        self.layer_num = layer_num
        
        # å±‚çº§æƒé‡å­¦ä¹ å™¨
        self.layer_weights = nn.Linear(layer_num, 1, bias=False)
        
    def forward(self, waveform):
        # æå–å¤šå±‚ç‰¹å¾
        all_layer_outputs = []
        for i in range(self.layer_num):
            layer_output = self.model.extract_features(waveform, layer=i)
            all_layer_outputs.append(layer_output)
        
        # å­¦ä¹ æœ€ä¼˜å±‚çº§ç»„åˆ
        stacked_features = torch.stack(all_layer_outputs, dim=-1)  # [B, T, D, L]
        weights = self.layer_weights.weight  # [1, L]
        weighted_features = torch.matmul(stacked_features, weights.t())  # [B, T, D]
        
        return weighted_features
```

**WavLMå…³é”®ç‰¹æ€§**:
- **13å±‚Transformerç¼–ç å™¨**
- **768ç»´ç‰¹å¾è¾“å‡º**
- **è‡ªç›‘ç£é¢„è®­ç»ƒ**: åœ¨æµ·é‡æ— æ ‡æ³¨éŸ³é¢‘ä¸Šè®­ç»ƒ
- **å¤šä»»åŠ¡å­¦ä¹ **: åŒæ—¶å­¦ä¹ å†…å®¹å’Œè¯´è¯äººç‰¹å¾

##### Conformerç¼–ç å™¨è¯¦è§£

Conformerç»“åˆäº†CNNçš„å±€éƒ¨å»ºæ¨¡å’ŒTransformerçš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼š

```python
class ConformerEncoder(nn.Module):
    def __init__(self, attention_in=256, ffn_hidden=1024, num_head=4, num_layer=4):
        super().__init__()
        
        self.layers = nn.ModuleList([
            ConformerBlock(
                dim=attention_in,
                ffn_dim=ffn_hidden,
                num_heads=num_head,
                conv_kernel_size=31,
                dropout=0.1
            ) for _ in range(num_layer)
        ])
        
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)  # æ¯ä¸ªConformerå—çš„å¤„ç†
        return x
```

**ConformerBlockç»“æ„**:
```
è¾“å…¥ç‰¹å¾
    â†“
å¤šå¤´è‡ªæ³¨æ„åŠ› (Multi-Head Self Attention)
    â†“
æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    â†“
å‰é¦ˆç½‘ç»œ (Feed Forward Network)
    â†“
æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    â†“
å·ç§¯æ¨¡å— (Convolution Module)
    â†“
æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    â†“
è¾“å‡º
```

##### å¹‚é›†ç¼–ç  (Powerset Encoding)

DiariZené‡‡ç”¨å¹‚é›†ç¼–ç æ¥å¤„ç†å¤šè¯´è¯äººåœºæ™¯ï¼š

```python
class PowersetEncoding:
    def __init__(self, max_speakers=4):
        self.max_speakers = max_speakers
        self.num_classes = 2 ** max_speakers  # 2^4 = 16ä¸ªç±»åˆ«
        
    def encode(self, speaker_labels):
        """å°†è¯´è¯äººæ ‡ç­¾ç¼–ç ä¸ºå¹‚é›†ç´¢å¼•"""
        # speaker_labels: [B, T, max_speakers] äºŒè¿›åˆ¶çŸ©é˜µ
        # è¿”å›: [B, T] ç±»åˆ«ç´¢å¼• (0-15)
        
        powers = 2 ** torch.arange(self.max_speakers)
        indices = torch.sum(speaker_labels * powers, dim=-1)
        return indices
        
    def decode(self, class_logits):
        """å°†åˆ†ç±»logitsè§£ç ä¸ºè¯´è¯äººæ¦‚ç‡"""
        # class_logits: [B, T, num_classes]
        # è¿”å›: [B, T, max_speakers] è¯´è¯äººå­˜åœ¨æ¦‚ç‡
        
        # å°†æ¯ä¸ªç±»åˆ«æ˜ å°„å›äºŒè¿›åˆ¶å‘é‡
        binary_matrix = self._logits_to_binary(class_logits)
        return binary_matrix
```

**å¹‚é›†ç¼–ç ä¼˜åŠ¿**:
- **æ˜¾å¼å»ºæ¨¡é‡å **: å¯ä»¥å‡†ç¡®è¡¨ç¤ºå¤šä¸ªè¯´è¯äººåŒæ—¶è¯´è¯
- **ç«¯åˆ°ç«¯è®­ç»ƒ**: æ— éœ€å¤æ‚çš„åå¤„ç†
- **è®¡ç®—æ•ˆç‡**: åˆ†ç±»ä»»åŠ¡æ¯”å›å½’ä»»åŠ¡æ›´ç¨³å®š

#### åŒä¼˜åŒ–å™¨è®­ç»ƒç­–ç•¥

DiariZené‡‡ç”¨åˆ›æ–°çš„åŒä¼˜åŒ–å™¨è®¾è®¡æ¥å¹³è¡¡é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒå’Œæ–°ä»»åŠ¡å­¦ä¹ ï¼š

```python
class DualOptimizerTrainer:
    def __init__(self, model):
        # å°å­¦ä¹ ç‡ä¼˜åŒ–å™¨ï¼šç”¨äºWavLMå¾®è°ƒ
        self.optimizer_small = AdamW(
            params=model.wavlm_model.parameters(),
            lr=2e-5,  # å¾®è°ƒå­¦ä¹ ç‡
            weight_decay=0.01
        )
        
        # å¤§å­¦ä¹ ç‡ä¼˜åŒ–å™¨ï¼šç”¨äºæ–°ç»„ä»¶è®­ç»ƒ
        self.optimizer_big = AdamW(
            params=model.non_wavlm_parameters(),
            lr=1e-3,   # ä»å¤´è®­ç»ƒå­¦ä¹ ç‡
            weight_decay=0.01
        )
    
    def step(self, loss):
        # å°ä¼˜åŒ–å™¨æ­¥éª¤
        self.optimizer_small.zero_grad()
        loss.backward(retain_graph=True)
        self.optimizer_small.step()
        
        # å¤§ä¼˜åŒ–å™¨æ­¥éª¤
        self.optimizer_big.zero_grad()
        loss.backward()
        self.optimizer_big.step()
```

**è®¾è®¡åŸç†**:
- **é¢„è®­ç»ƒéƒ¨åˆ†(WavLM)**: å·²ç»å­¦ä¼šé€šç”¨è¯­éŸ³è¡¨ç¤ºï¼Œåªéœ€å°å¹…è°ƒæ•´
- **æ–°å¢éƒ¨åˆ†(Conformer+åˆ†ç±»å™¨)**: ä»é›¶å¼€å§‹å­¦ä¹ è¯´è¯äººåˆ†ç¦»ä»»åŠ¡
- **å­¦ä¹ ç‡å·®å¼‚**: ç›¸å·®50å€ï¼Œé€‚åº”ä¸åŒç»„ä»¶çš„å­¦ä¹ éœ€æ±‚

---

### è®­ç»ƒæµç¨‹è¯¦è§£

#### æ•°æ®å‡†å¤‡æµç¨‹

DiariZençš„è®­ç»ƒéœ€è¦ä¸‰ç§æ ¸å¿ƒæ•°æ®æ–‡ä»¶ï¼š

**1. éŸ³é¢‘åˆ—è¡¨æ–‡ä»¶ (wav.scp)**:
```
session_id1 /path/to/audio1.wav
session_id2 /path/to/audio2.wav
session_id3 /path/to/audio3.wav
```

**2. æ ‡æ³¨æ–‡ä»¶ (rttm)**:
```
SPEAKER session_id1 1 0.00 2.50 <NA> <NA> spk1 <NA> <NA>
SPEAKER session_id1 1 2.50 1.80 <NA> <NA> spk2 <NA> <NA>
SPEAKER session_id1 1 4.30 3.20 <NA> <NA> spk1 <NA> <NA>
```

**RTTMæ ¼å¼è¯¦è§£**:
- **SPEAKER**: è®°å½•ç±»å‹æ ‡è¯†ç¬¦
- **session_id1**: ä¼šè¯ID
- **1**: æ–‡ä»¶ç¼–å·ï¼ˆé€šå¸¸ä¸º1ï¼‰
- **0.00**: å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
- **2.50**: æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
- **spk1**: è¯´è¯äººæ ‡ç­¾

**3. è¯„ä¼°æ®µæ ‡è®° (uem)**:
```
session_id1 1 0.00 30.00
session_id2 1 0.00 45.20
```
å®šä¹‰éœ€è¦è¯„ä¼°çš„æ—¶é—´æ®µã€‚

#### æ•°æ®åŠ è½½å’Œé¢„å¤„ç†

```python
class DiarizationDataset(torch.utils.data.Dataset):
    def __init__(self, scp_file, rttm_file, uem_file, chunk_size=8, chunk_shift=6):
        self.chunk_size = chunk_size  # å—å¤§å°ï¼ˆç§’ï¼‰
        self.chunk_shift = chunk_shift  # å—åç§»ï¼ˆç§’ï¼‰
        self.sample_rate = 16000
        
        # åŠ è½½éŸ³é¢‘è·¯å¾„
        self.audio_paths = self.load_scp(scp_file)
        
        # åŠ è½½æ ‡æ³¨
        self.annotations = self.load_rttm(rttm_file)
        
        # åŠ è½½è¯„ä¼°æ®µ
        self.uem_segments = self.load_uem(uem_file)
        
        # ç”Ÿæˆè®­ç»ƒå—
        self.chunks = self.generate_chunks()
    
    def __getitem__(self, idx):
        chunk_info = self.chunks[idx]
        
        # åŠ è½½éŸ³é¢‘å—
        audio_chunk = self.load_audio_chunk(chunk_info)
        
        # åŠ è½½å¯¹åº”æ ‡æ³¨
        labels = self.load_labels_for_chunk(chunk_info)
        
        return audio_chunk, labels
```

#### è®­ç»ƒå¾ªç¯è¯¦è§£

```python
def training_epoch(model, dataloader, optimizer_small, optimizer_big, device):
    model.train()
    
    for batch_idx, (waveforms, labels) in enumerate(dataloader):
        waveforms = waveforms.to(device)  # [B, 1, T]
        labels = labels.to(device)        # [B, T, num_classes]
        
        # å‰å‘ä¼ æ’­
        logits = model(waveforms)  # [B, T, num_classes]
        
        # è®¡ç®—æŸå¤±
        loss = F.binary_cross_entropy_with_logits(
            logits.view(-1, logits.size(-1)),
            labels.view(-1, labels.size(-1))
        )
        
        # åŒä¼˜åŒ–å™¨æ›´æ–°
        optimizer_small.zero_grad()
        optimizer_big.zero_grad()
        
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer_small.step()
        optimizer_big.step()
        
        # æ—¥å¿—è®°å½•
        if batch_idx % 100 == 0:
            print(f"Batch {batch_idx}, Loss: {loss.item():.4f}")
```

#### éªŒè¯æµç¨‹

```python
def validation_epoch(model, dataloader, device):
    model.eval()
    
    total_der = 0
    num_sessions = 0
    
    with torch.no_grad():
        for waveforms, labels, session_ids in dataloader:
            # æ‰¹é‡æ¨ç†
            predictions = model(waveforms.to(device))
            
            # è®¡ç®—DER
            for pred, label, session_id in zip(predictions, labels, session_ids):
                der = calculate_der_for_session(pred, label)
                total_der += der
                num_sessions += 1
    
    avg_der = total_der / num_sessions
    return avg_der
```

---

### æ¨ç†æµç¨‹è¯¦è§£

#### DiariZenPipelineæ¶æ„

DiariZenPipelineç»§æ‰¿è‡ªpyannote-audioçš„SpeakerDiarizationç®¡é“ï¼Œå¹¶å®šåˆ¶äº†æ¨ç†æµç¨‹ï¼š

```python
class DiariZenPipeline(SpeakerDiarization):
    def __init__(self, diarizen_hub, embedding_model, config_parse=None):
        # åŠ è½½DiariZenåˆ†å‰²æ¨¡å‹
        segmentation_model = self.load_segmentation_model(diarizen_hub)
        
        # åˆå§‹åŒ–çˆ¶ç±»
        super().__init__(
            segmentation=segmentation_model,
            embedding=embedding_model,
            clustering="VBxClustering",  # æˆ– "AgglomerativeClustering"
            embedding_exclude_overlap=True,
            device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
        )
        
        # è‡ªå®šä¹‰å‚æ•°
        self.apply_median_filtering = True
        self.min_speakers = 1
        self.max_speakers = 20
```

#### æ¨ç†æ­¥éª¤è¯¦è§£

**æ­¥éª¤1: éŸ³é¢‘é¢„å¤„ç†**
```python
def preprocess_audio(self, audio_file):
    """éŸ³é¢‘é¢„å¤„ç†"""
    # åŠ è½½éŸ³é¢‘
    waveform, sample_rate = torchaudio.load(audio_file)
    
    # é‡é‡‡æ ·åˆ°16kHz
    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(sample_rate, 16000)
        waveform = resampler(waveform)
    
    # è½¬æ¢ä¸ºå•å£°é“
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)
    
    return waveform.squeeze(0)  # [T]
```

**æ­¥éª¤2: æ»‘åŠ¨çª—å£åˆ†å‰²**
```python
def sliding_window_segmentation(self, waveform, step_ratio=0.1):
    """æ»‘åŠ¨çª—å£åˆ†å‰²æ¨ç†"""
    
    # æ¨¡å‹æœŸæœ›çš„å—å¤§å°
    chunk_duration = self.segmentation.model.chunk_size  # 8ç§’
    sample_rate = self.segmentation.model.sample_rate     # 16000Hz
    chunk_samples = int(chunk_duration * sample_rate)    # 128000æ ·æœ¬
    
    # æ»‘åŠ¨æ­¥é•¿
    step_samples = int(chunk_samples * step_ratio)  # 12800æ ·æœ¬
    
    segments = []
    start_sample = 0
    
    while start_sample < len(waveform):
        end_sample = min(start_sample + chunk_samples, len(waveform))
        
        # æå–éŸ³é¢‘å—
        chunk = waveform[start_sample:end_sample]
        
        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        if len(chunk) < chunk_samples:
            padding = torch.zeros(chunk_samples - len(chunk))
            chunk = torch.cat([chunk, padding])
        
        segments.append({
            'audio': chunk,
            'start_time': start_sample / sample_rate,
            'end_time': end_sample / sample_rate
        })
        
        start_sample += step_samples
    
    return segments
```

**æ­¥éª¤3: æ‰¹é‡åˆ†å‰²æ¨ç†**
```python
def batch_segmentation_inference(self, audio_segments, batch_size=32):
    """æ‰¹é‡åˆ†å‰²æ¨¡å‹æ¨ç†"""
    
    all_segmentations = []
    
    for i in range(0, len(audio_segments), batch_size):
        batch_segments = audio_segments[i:i+batch_size]
        
        # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
        batch_audio = torch.stack([seg['audio'] for seg in batch_segments])
        batch_audio = batch_audio.unsqueeze(1)  # [B, 1, T]
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            batch_logits = self.segmentation.model(batch_audio.to(self.device))
            batch_probs = torch.sigmoid(batch_logits)
        
        # è§£ç ä¸ºåˆ†å‰²ç»“æœ
        for j, seg in enumerate(batch_segments):
            probs = batch_probs[j]  # [T, num_classes]
            
            # å¹‚é›†è§£ç ä¸ºè¯´è¯äººæ´»åŠ¨
            speaker_activities = self.decode_powerset(probs)
            
            segmentation = {
                'start_time': seg['start_time'],
                'end_time': seg['end_time'],
                'speaker_activities': speaker_activities  # [T, max_speakers]
            }
            
            all_segmentations.append(segmentation)
    
    return all_segmentations
```

**æ­¥éª¤4: åµŒå…¥æå–**
```python
def extract_embeddings(self, segmentations, waveform):
    """æå–è¯´è¯äººåµŒå…¥"""
    
    embeddings = []
    segments_for_embedding = []
    
    for seg in segmentations:
        # æ‰¾åˆ°æœ‰å£°æ®µ
        speaker_activities = seg['speaker_activities']
        
        for speaker_idx in range(speaker_activities.shape[1]):
            speaker_activity = speaker_activities[:, speaker_idx]
            
            # æ£€æµ‹è¯´è¯äººæ´»åŠ¨æ®µ
            active_frames = speaker_activity > 0.5
            if active_frames.sum() > 0:
                # è®¡ç®—æ—¶é—´è¾¹ç•Œ
                start_frame = active_frames.nonzero()[0, 0]
                end_frame = active_frames.nonzero()[-1, 0]
                
                start_time = seg['start_time'] + start_frame * self.segmentation.model.get_rf_info()[2]
                end_time = seg['start_time'] + (end_frame + 1) * self.segmentation.model.get_rf_info()[2]
                
                segments_for_embedding.append({
                    'start': start_time,
                    'end': end_time,
                    'speaker_idx': speaker_idx
                })
    
    # æ‰¹é‡æå–åµŒå…¥
    if segments_for_embedding:
        batch_embeddings = self.embedding_model(waveform, segments_for_embedding)
        embeddings.extend(batch_embeddings)
    
    return embeddings, segments_for_embedding
```

**æ­¥éª¤5: èšç±»å’Œåå¤„ç†**
```python
def clustering_and_postprocessing(self, embeddings, segments):
    """èšç±»å’Œåå¤„ç†"""
    
    # å‡†å¤‡èšç±»è¾“å…¥
    embedding_vectors = torch.stack([emb['embedding'] for emb in embeddings])
    
    # æ‰§è¡Œèšç±»
    if self.clustering_method == "VBxClustering":
        cluster_labels = self.vbx_clustering(embedding_vectors)
    else:
        cluster_labels = self.agglomerative_clustering(embedding_vectors)
    
    # ç”Ÿæˆæœ€ç»ˆç»“æœ
    diarization_result = self.create_diarization_annotation(
        segments, cluster_labels
    )
    
    # ä¸­å€¼æ»¤æ³¢å¹³æ»‘
    if self.apply_median_filtering:
        diarization_result = self.median_filter(diarization_result)
    
    return diarization_result
```

#### æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

**1. æ‰¹å¤„ç†ä¼˜åŒ–**
```python
def optimized_batch_inference(self, audio_file, batch_size=64):
    """ä¼˜åŒ–çš„æ‰¹å¤„ç†æ¨ç†"""
    
    # é¢„åŠ è½½å’Œé¢„å¤„ç†
    waveform = self.preprocess_audio(audio_file)
    audio_segments = self.sliding_window_segmentation(waveform)
    
    # ä½¿ç”¨æ›´å¤§çš„æ‰¹å¤„ç†å¤§å°
    all_segmentations = self.batch_segmentation_inference(
        audio_segments, batch_size=batch_size
    )
    
    # å¹¶è¡ŒåµŒå…¥æå–
    embeddings, segments = self.parallel_embedding_extraction(
        all_segmentations, waveform
    )
    
    return self.clustering_and_postprocessing(embeddings, segments)
```

**2. å†…å­˜ç®¡ç†**
```python
def memory_efficient_inference(self, audio_file):
    """å†…å­˜é«˜æ•ˆæ¨ç†"""
    
    # åˆ†å—å¤„ç†é•¿éŸ³é¢‘
    waveform = self.preprocess_audio(audio_file)
    max_chunk_duration = 300  # 5åˆ†é’Ÿå—
    
    results = []
    for start_time in range(0, len(waveform) // 16000, max_chunk_duration):
        end_time = min(start_time + max_chunk_duration, len(waveform) // 16000)
        
        # å¤„ç†éŸ³é¢‘å—
        chunk_waveform = waveform[start_time*16000:end_time*16000]
        chunk_result = self.process_chunk(chunk_waveform, start_time)
        results.append(chunk_result)
        
        # æ¸…ç†GPUå†…å­˜
        torch.cuda.empty_cache()
    
    # åˆå¹¶ç»“æœ
    return self.merge_chunk_results(results)
```

---

## ç¯å¢ƒé…ç½®ä¸å®‰è£…

### ç³»ç»Ÿè¦æ±‚
- **æ“ä½œç³»ç»Ÿ**ï¼šLinux/macOS/Windows
- **Pythonç‰ˆæœ¬**ï¼šâ‰¥ 3.10
- **GPU**ï¼šæ¨èNVIDIA GPUï¼ˆæ”¯æŒCUDA 12.1ï¼‰
- **å†…å­˜**ï¼šâ‰¥ 16GB RAM
- **å­˜å‚¨**ï¼šâ‰¥ 10GB å¯ç”¨ç©ºé—´

### å®‰è£…æ­¥éª¤

#### 1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
```bash
# ä½¿ç”¨condaåˆ›å»ºç¯å¢ƒ
conda create --name diarizen python=3.10
conda activate diarizen
```

#### 2. å®‰è£…PyTorch
```bash
# æ ¹æ®ä½ çš„CUDAç‰ˆæœ¬è°ƒæ•´
conda install pytorch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 pytorch-cuda=12.1 -c pytorch -c nvidia

# å¦‚æœæ²¡æœ‰GPUï¼Œä½¿ç”¨CPUç‰ˆæœ¬
# conda install pytorch==2.1.1 torchvision==0.16.1 torchaudio==2.1.1 cpuonly -c pytorch
```

#### 3. å®‰è£…DiariZen
```bash
# å…‹éš†ä»“åº“
git clone https://github.com/BUTSpeechFIT/DiariZen.git
cd DiariZen

# å®‰è£…ä¾èµ–
pip install -r requirements.txt && pip install -e .

# å®‰è£…pyannote-audio
cd pyannote-audio && pip install -e .[dev,testing]
cd ..

# åˆå§‹åŒ–å­æ¨¡å—
git submodule init
git submodule update
```

#### 4. éªŒè¯å®‰è£…
```python
# æµ‹è¯•å®‰è£…æ˜¯å¦æˆåŠŸ
from diarizen.pipelines.inference import DiariZenPipeline
print("DiariZenå®‰è£…æˆåŠŸï¼")
```

### Dockerå®‰è£…ï¼ˆæ¨èï¼‰
```dockerfile
# Dockerfileç¤ºä¾‹
FROM pytorch/pytorch:2.1.1-cuda12.1-cudnn8-devel

WORKDIR /workspace
COPY . .

RUN pip install -r requirements.txt && pip install -e .
RUN cd pyannote-audio && pip install -e .[dev,testing]

CMD ["python", "-c", "from diarizen.pipelines.inference import DiariZenPipeline; print('Ready!')"]
```

---

## æ ¸å¿ƒæ¶æ„åŸç†

### æ•´ä½“æµç¨‹å›¾
```mermaid
graph TB
    A[éŸ³é¢‘è¾“å…¥] --> B[WavLMç‰¹å¾æå–]
    B --> C[Conformerç¼–ç å™¨]
    C --> D[è¯­éŸ³æ´»åŠ¨æ£€æµ‹VAD]
    C --> E[è¯´è¯äººåµŒå…¥æå–]
    D --> F[æ—¶é—´åˆ†æ®µ]
    E --> G[åµŒå…¥å‘é‡]
    F --> H[èšç±»ç®—æ³•]
    G --> H
    H --> I[åå¤„ç†]
    I --> J[RTTMè¾“å‡º]
```

### 1. ç‰¹å¾æå–æ¨¡å—

#### WavLMé¢„è®­ç»ƒæ¨¡å‹è¯¦è§£

**WavLMæ¶æ„**ï¼š
```python
class WavLMFeatureExtractor(nn.Module):
    def __init__(self, model_path, layer_num=13, feature_dim=768):
        super().__init__()

        # åŠ è½½é¢„è®­ç»ƒWavLMæ¨¡å‹
        self.wavlm = self.load_pretrained_wavlm(model_path)
        self.layer_num = layer_num
        self.feature_dim = feature_dim

        # å¯å­¦ä¹ çš„å¤šå±‚ç‰¹å¾èåˆ
        self.layer_weighting = nn.Linear(layer_num, 1, bias=False)

        # åˆå§‹åŒ–æƒé‡ä¸ºå‡åŒ€åˆ†å¸ƒ
        nn.init.uniform_(self.layer_weighting.weight, -0.1, 0.1)

    def forward(self, waveform):
        """
        Args:
            waveform: [B, T] åŸå§‹éŸ³é¢‘æ³¢å½¢
        Returns:
            features: [B, T', D] èåˆåçš„ç‰¹å¾
        """

        # æå–æ‰€æœ‰å±‚çš„ç‰¹å¾
        all_layer_features = []
        for layer_idx in range(self.layer_num):
            # WavLMå‰å‘ä¼ æ’­ï¼ŒæŒ‡å®šè¾“å‡ºå±‚
            layer_output = self.wavlm.extract_features(
                waveform,
                output_layer=layer_idx,
                mask=False  # æ¨ç†æ—¶å…³é—­masking
            )[0]  # [B, T', D]

            all_layer_features.append(layer_output)

        # å †å æ‰€æœ‰å±‚ç‰¹å¾: [B, T', D, L]
        stacked_features = torch.stack(all_layer_features, dim=-1)

        # å­¦ä¹ æœ€ä¼˜å±‚çº§æƒé‡: [L, 1] -> [1, L]
        layer_weights = self.layer_weighting.weight.t()
        layer_weights = F.softmax(layer_weights / 0.1, dim=0)  # æ¸©åº¦ç¼©æ”¾

        # åŠ æƒèåˆ: [B, T', D, L] * [L, 1] -> [B, T', D]
        fused_features = torch.matmul(stacked_features, layer_weights)

        return fused_features
```

**WavLMå…³é”®åˆ›æ–°**ï¼š
1. **å¤šä»»åŠ¡é¢„è®­ç»ƒ**ï¼š
   - æ©ç è¯­è¨€å»ºæ¨¡ (Masked Language Modeling)
   - å¯¹æ¯”å­¦ä¹  (Contrastive Learning)
   - å»å™ªè‡ªç¼–ç  (Denoising Autoencoding)

2. **åˆ†å±‚ç‰¹å¾è¡¨ç¤º**ï¼š
   - **åº•å±‚(0-3)**: å£°å­¦ç‰¹å¾ (phonetic features)
   - **ä¸­å±‚(4-8)**: è¯­ä¹‰ç‰¹å¾ (semantic features)
   - **é«˜å±‚(9-12)**: è¯´è¯äººç‰¹å¾ (speaker features)

3. **ç›¸å¯¹ä½ç½®ç¼–ç **ï¼š
   - æ¯”ç»å¯¹ä½ç½®ç¼–ç æ›´é€‚åˆå˜é•¿éŸ³é¢‘
   - æ”¯æŒä»»æ„é•¿åº¦çš„åºåˆ—æ¨ç†

#### Conformerç¼–ç å™¨è¯¦è§£

**Conformer Blockæ¶æ„**ï¼š
```python
class ConformerBlock(nn.Module):
    def __init__(self, dim=256, ffn_dim=1024, num_heads=4, kernel_size=31, dropout=0.1):
        super().__init__()

        # 1. å‰é¦ˆç½‘ç»œæ¨¡å— (Feed Forward Module)
        self.ffn1 = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, ffn_dim),
            nn.SiLU(),  # Swishæ¿€æ´»å‡½æ•°
            nn.Dropout(dropout),
            nn.Linear(ffn_dim, dim),
            nn.Dropout(dropout)
        )

        # 2. å¤šå¤´è‡ªæ³¨æ„åŠ›æ¨¡å— (Multi-Head Self Attention)
        self.self_attn = nn.MultiheadAttention(
            embed_dim=dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )
        self.self_attn_norm = nn.LayerNorm(dim)

        # 3. å·ç§¯æ¨¡å— (Convolution Module)
        self.conv_module = ConvolutionModule(
            dim=dim,
            kernel_size=kernel_size,
            dropout=dropout
        )

        # 4. ç¬¬äºŒå‰é¦ˆç½‘ç»œæ¨¡å—
        self.ffn2 = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, ffn_dim),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(ffn_dim, dim),
            nn.Dropout(dropout)
        )

    def forward(self, x, mask=None):
        """
        Args:
            x: [B, T, D] è¾“å…¥ç‰¹å¾
            mask: [B, T] æ³¨æ„åŠ›æ©ç 
        Returns:
            x: [B, T, D] è¾“å‡ºç‰¹å¾
        """

        # æ®‹å·®è¿æ¥ + å‰é¦ˆç½‘ç»œ1
        x = x + 0.5 * self.ffn1(x)

        # æ®‹å·®è¿æ¥ + å¤šå¤´è‡ªæ³¨æ„åŠ›
        attn_out, _ = self.self_attn(
            query=x, key=x, value=x,
            key_padding_mask=mask
        )
        x = x + attn_out
        x = self.self_attn_norm(x)

        # æ®‹å·®è¿æ¥ + å·ç§¯æ¨¡å—
        x = x + self.conv_module(x)

        # æ®‹å·®è¿æ¥ + å‰é¦ˆç½‘ç»œ2
        x = x + 0.5 * self.ffn2(x)

        return x
```

**ConvolutionModuleå®ç°**ï¼š
```python
class ConvolutionModule(nn.Module):
    def __init__(self, dim, kernel_size=31, dropout=0.1):
        super().__init__()

        # å±‚å½’ä¸€åŒ–
        self.layer_norm = nn.LayerNorm(dim)

        # é€ç‚¹å·ç§¯
        self.pointwise_conv1 = nn.Conv1d(
            dim, 2 * dim, kernel_size=1, stride=1, padding=0
        )
        self.pointwise_conv2 = nn.Conv1d(
            dim, dim, kernel_size=1, stride=1, padding=0
        )

        # æ·±åº¦å·ç§¯
        self.depthwise_conv = nn.Conv1d(
            dim, dim, kernel_size=kernel_size, stride=1,
            padding=(kernel_size - 1) // 2, groups=dim
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        """
        Args:
            x: [B, T, D]
        Returns:
            x: [B, T, D]
        """

        # è½¬æ¢ä¸ºå·ç§¯æ ¼å¼: [B, D, T]
        x = x.transpose(1, 2)

        # å±‚å½’ä¸€åŒ–
        x = self.layer_norm(x.transpose(1, 2)).transpose(1, 2)

        # é€ç‚¹å·ç§¯ + GLUæ¿€æ´»
        x = self.pointwise_conv1(x)
        x = F.glu(x, dim=1)  # åˆ†å‰²å¹¶åº”ç”¨GLU

        # æ·±åº¦å·ç§¯
        x = self.depthwise_conv(x)

        # é€ç‚¹å·ç§¯ + Dropout
        x = self.pointwise_conv2(x)
        x = self.dropout(x)

        # è½¬å›åŸå§‹æ ¼å¼: [B, T, D]
        x = x.transpose(1, 2)

        return x
```

**Conformerä¼˜åŠ¿åˆ†æ**ï¼š
1. **å±€éƒ¨å»ºæ¨¡**ï¼šé€šè¿‡æ·±åº¦å¯åˆ†ç¦»å·ç§¯æ•æ‰å±€éƒ¨æ—¶åºæ¨¡å¼
2. **å…¨å±€å»ºæ¨¡**ï¼šé€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡é•¿è·ç¦»ä¾èµ–
3. **å‚æ•°æ•ˆç‡**ï¼šç›¸æ¯”çº¯Transformerå‡å°‘è®¡ç®—å¤æ‚åº¦
4. **å¹¶è¡Œå‹å¥½**ï¼šå·ç§¯æ“ä½œå¤©ç„¶æ”¯æŒå¹¶è¡Œè®¡ç®—

### 2. åŒä¼˜åŒ–å™¨è®­ç»ƒç­–ç•¥è¯¦è§£

DiariZençš„åŒä¼˜åŒ–å™¨è®¾è®¡æ˜¯å…¶æ ¸å¿ƒåˆ›æ–°ä¹‹ä¸€ï¼Œé€šè¿‡ä¸åŒçš„å­¦ä¹ ç‡ç­–ç•¥åˆ†åˆ«å¤„ç†é¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒå’Œæ–°ä»»åŠ¡å­¦ä¹ ï¼š

#### ä¼˜åŒ–å™¨é…ç½®è¯¦è§£

```python
class DualOptimizerScheduler:
    def __init__(self, model, config):
        # å‚æ•°åˆ†ç»„
        wavlm_params = list(model.wavlm_model.parameters())
        conformer_params = list(model.conformer.parameters())
        classifier_params = list(model.classifier.parameters())

        # å°å­¦ä¹ ç‡ç»„ï¼šWavLMé¢„è®­ç»ƒå‚æ•°
        self.optimizer_small = AdamW([
            {'params': wavlm_params, 'lr': config['lr_small'], 'weight_decay': 0.01}
        ], betas=(0.9, 0.98), eps=1e-8)

        # å¤§å­¦ä¹ ç‡ç»„ï¼šæ–°å¢ç½‘ç»œå‚æ•°
        self.optimizer_big = AdamW([
            {'params': conformer_params, 'lr': config['lr_big'], 'weight_decay': 0.01},
            {'params': classifier_params, 'lr': config['lr_big'], 'weight_decay': 0.01},
            {'params': model.layer_weighting.parameters(), 'lr': config['lr_big'], 'weight_decay': 0.01}
        ], betas=(0.9, 0.98), eps=1e-8)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler_small = self.create_scheduler(
            self.optimizer_small, config['warmup_steps'], config['total_steps']
        )
        self.scheduler_big = self.create_scheduler(
            self.optimizer_big, config['warmup_steps'], config['total_steps']
        )

    def create_scheduler(self, optimizer, warmup_steps, total_steps):
        """åˆ›å»ºå¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«è°ƒåº¦å™¨"""
        def lr_lambda(step):
            if step < warmup_steps:
                return step / max(1, warmup_steps)
            else:
                progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)
                return 0.5 * (1 + math.cos(math.pi * progress))

        return LambdaLR(optimizer, lr_lambda)

    def step(self, loss):
        """æ‰§è¡Œä¼˜åŒ–æ­¥éª¤"""
        # æ¸…ç©ºæ¢¯åº¦
        self.optimizer_small.zero_grad()
        self.optimizer_big.zero_grad()

        # åå‘ä¼ æ’­
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.optimizer_small.param_groups[0]['params'], max_norm=1.0)
        torch.nn.utils.clip_grad_norm_(self.optimizer_big.param_groups[0]['params'], max_norm=1.0)

        # å‚æ•°æ›´æ–°
        self.optimizer_small.step()
        self.optimizer_big.step()

        # å­¦ä¹ ç‡æ›´æ–°
        self.scheduler_small.step()
        self.scheduler_big.step()
```

#### å­¦ä¹ ç‡ç­–ç•¥åˆ†æ

**WavLMå­¦ä¹ ç‡æ›²çº¿**ï¼š
```python
def plot_lr_curves():
    """å¯è§†åŒ–å­¦ä¹ ç‡å˜åŒ–"""

    # è®­ç»ƒé…ç½®
    config = {
        'lr_small': 2e-5,    # WavLMå­¦ä¹ ç‡
        'lr_big': 1e-3,      # Conformerå­¦ä¹ ç‡
        'warmup_steps': 1000,
        'total_steps': 50000
    }

    steps = np.arange(50000)
    lr_small = []
    lr_big = []

    for step in steps:
        # é¢„çƒ­é˜¶æ®µ
        if step < config['warmup_steps']:
            factor = step / config['warmup_steps']
            lr_small.append(config['lr_small'] * factor)
            lr_big.append(config['lr_big'] * factor)
        else:
            # ä½™å¼¦é€€ç«é˜¶æ®µ
            progress = (step - config['warmup_steps']) / (config['total_steps'] - config['warmup_steps'])
            cos_factor = 0.5 * (1 + math.cos(math.pi * progress))

            lr_small.append(config['lr_small'] * cos_factor)
            lr_big.append(config['lr_big'] * cos_factor)

    plt.figure(figsize=(12, 6))
    plt.plot(steps, lr_small, label='WavLM LR (2e-5)', linewidth=2)
    plt.plot(steps, lr_big, label='Conformer LR (1e-3)', linewidth=2)
    plt.xlabel('Training Steps')
    plt.ylabel('Learning Rate')
    plt.yscale('log')
    plt.legend()
    plt.title('Dual Optimizer Learning Rate Schedule')
    plt.grid(True, alpha=0.3)
    plt.show()
```

**å­¦ä¹ ç‡å·®å¼‚çš„ç†è®ºä¾æ®**ï¼š
1. **é¢„è®­ç»ƒæ¨¡å‹ç¨³å®šæ€§**ï¼šWavLMå·²ç»åœ¨æµ·é‡æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œå‚æ•°å·²ç»æ¥è¿‘æœ€ä¼˜
2. **ç¾éš¾æ€§é—å¿˜**ï¼šå¤§å­¦ä¹ ç‡å¯èƒ½ç ´åå·²å­¦åˆ°çš„é€šç”¨è¯­éŸ³è¡¨ç¤º
3. **ä»»åŠ¡ç›¸å…³æ€§**ï¼šWavLMå­¦ä¹ é€šç”¨ç‰¹å¾ï¼ŒConformerå­¦ä¹ ä»»åŠ¡ç‰¹å®šç‰¹å¾
4. **æ¢¯åº¦å°ºåº¦å·®å¼‚**ï¼šä¸åŒç»„ä»¶çš„æ¢¯åº¦å¹…åº¦å¯èƒ½ç›¸å·®å¾ˆå¤§

### 3. æŸå¤±å‡½æ•°è®¾è®¡è¯¦è§£

#### å¤šæ ‡ç­¾äºŒå…ƒäº¤å‰ç†µæŸå¤±

**å¹‚é›†åˆ†ç±»çš„æ ¸å¿ƒæŸå¤±å‡½æ•°**ï¼š

```python
class PowersetBCEWithLogitsLoss(nn.Module):
    def __init__(self, reduction='mean', pos_weight=None):
        super().__init__()
        self.reduction = reduction
        self.pos_weight = pos_weight

    def forward(self, logits, targets):
        """
        Args:
            logits: [B, T, num_classes] æ¨¡å‹é¢„æµ‹logits
            targets: [B, T, num_classes] ç›®æ ‡æ ‡ç­¾ï¼ˆone-hotç¼–ç ï¼‰
        Returns:
            loss: æ ‡é‡æŸå¤±å€¼
        """

        # å±•å¹³ä¸º[B*T, num_classes]
        logits_flat = logits.view(-1, logits.size(-1))
        targets_flat = targets.view(-1, targets.size(-1))

        # è®¡ç®—BCEæŸå¤±
        loss_fn = nn.BCEWithLogitsLoss(
            reduction='none',
            pos_weight=self.pos_weight
        )

        # é€å…ƒç´ æŸå¤±: [B*T, num_classes]
        element_loss = loss_fn(logits_flat, targets_flat)

        # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        if self.pos_weight is not None:
            # ä¸ºæ­£æ ·æœ¬ç±»åˆ«èµ‹äºˆæ›´é«˜æƒé‡
            positive_mask = targets_flat == 1
            element_loss = torch.where(
                positive_mask,
                element_loss * self.pos_weight.unsqueeze(0),
                element_loss
            )

        # æŸå¤±èšåˆ
        if self.reduction == 'mean':
            return element_loss.mean()
        elif self.reduction == 'sum':
            return element_loss.sum()
        else:
            return element_loss.view(logits.shape)
```

**ç±»åˆ«æƒé‡è®¾è®¡**ï¼š
```python
def compute_class_weights(dataset):
    """è®¡ç®—ç±»åˆ«å¹³è¡¡æƒé‡"""

    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å‡ºç°é¢‘ç‡
    class_counts = torch.zeros(16)  # 2^4 = 16ä¸ªç±»åˆ«

    for _, labels in dataset:
        # labels: [T, num_classes] one-hot
        class_indices = labels.argmax(dim=-1)  # [T]
        for class_idx in class_indices:
            class_counts[class_idx] += 1

    # è®¡ç®—æƒé‡ï¼ˆé€†é¢‘ç‡åŠ æƒï¼‰
    total_samples = class_counts.sum()
    class_weights = total_samples / (class_counts + 1e-6)

    # å½’ä¸€åŒ–
    class_weights = class_weights / class_weights.mean()

    return class_weights
```

#### çŸ¥è¯†è’¸é¦æŸå¤±ï¼ˆæ¨¡å‹å‹ç¼©ï¼‰

**æ•™å¸ˆ-å­¦ç”Ÿè’¸é¦æ¡†æ¶**ï¼š

```python
class DistillationLoss(nn.Module):
    def __init__(self, temperature=2.0, alpha=0.7):
        super().__init__()
        self.temperature = temperature
        self.alpha = alpha  # è’¸é¦æŸå¤±æƒé‡

    def forward(self, student_logits, teacher_logits, targets):
        """
        Args:
            student_logits: [B, T, C] å­¦ç”Ÿæ¨¡å‹è¾“å‡º
            teacher_logits: [B, T, C] æ•™å¸ˆæ¨¡å‹è¾“å‡º
            targets: [B, T, C] çœŸå®æ ‡ç­¾
        Returns:
            loss: æ€»æŸå¤±
        """

        # 1. ç¡¬æ ‡ç­¾æŸå¤±ï¼ˆå­¦ç”Ÿæ¨¡å‹åˆ†ç±»æŸå¤±ï¼‰
        hard_loss = F.binary_cross_entropy_with_logits(
            student_logits, targets, reduction='mean'
        )

        # 2. è½¯æ ‡ç­¾æŸå¤±ï¼ˆè’¸é¦æŸå¤±ï¼‰
        # æ¸©åº¦ç¼©æ”¾çš„softmax
        student_soft = F.softmax(student_logits / self.temperature, dim=-1)
        teacher_soft = F.softmax(teacher_logits / self.temperature, dim=-1)

        # KLæ•£åº¦æŸå¤±
        distill_loss = F.kl_div(
            F.log_softmax(student_logits / self.temperature, dim=-1),
            F.softmax(teacher_logits / self.temperature, dim=-1),
            reduction='batchmean'
        ) * (self.temperature ** 2)

        # 3. ç‰¹å¾å¯¹é½æŸå¤±ï¼ˆå¯é€‰ï¼‰
        feature_loss = F.mse_loss(
            F.normalize(student_logits, dim=-1),
            F.normalize(teacher_logits, dim=-1).detach()
        )

        # æ€»æŸå¤±
        total_loss = (1 - self.alpha) * hard_loss + self.alpha * distill_loss

        return total_loss, {
            'hard_loss': hard_loss.item(),
            'distill_loss': distill_loss.item(),
            'total_loss': total_loss.item()
        }
```

#### æ—¶åºä¸€è‡´æ€§æŸå¤±

**å¢å¼ºæ—¶é—´è¿ç»­æ€§çš„æ­£åˆ™åŒ–é¡¹**ï¼š

```python
class TemporalConsistencyLoss(nn.Module):
    def __init__(self, weight=0.1, kernel_size=5):
        super().__init__()
        self.weight = weight
        self.kernel_size = kernel_size

        # é«˜æ–¯å¹³æ»‘æ ¸
        self.register_buffer('gaussian_kernel', self._create_gaussian_kernel())

    def _create_gaussian_kernel(self):
        """åˆ›å»ºé«˜æ–¯å¹³æ»‘æ ¸"""
        coords = torch.arange(self.kernel_size, dtype=torch.float32)
        coords -= self.kernel_size // 2

        # 1Dé«˜æ–¯æ ¸
        sigma = self.kernel_size / 6.0  # ç»éªŒå€¼
        g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))
        g /= g.sum()

        return g.view(1, 1, -1)

    def forward(self, predictions):
        """
        Args:
            predictions: [B, T, C] æ¨¡å‹é¢„æµ‹
        Returns:
            loss: æ—¶åºä¸€è‡´æ€§æŸå¤±
        """

        # è½¬æ¢ä¸ºæ¦‚ç‡ç©ºé—´
        probs = torch.sigmoid(predictions)

        # æ—¶é—´ç»´åº¦å¹³æ»‘
        smoothed_probs = F.conv1d(
            probs.transpose(1, 2),  # [B, C, T]
            self.gaussian_kernel,
            padding=self.kernel_size // 2
        ).transpose(1, 2)  # [B, T, C]

        # è®¡ç®—å¹³æ»‘å‰åå·®å¼‚
        consistency_loss = F.mse_loss(probs, smoothed_probs)

        return self.weight * consistency_loss
```

#### å¤šä»»åŠ¡æŸå¤±ç»„åˆ

**å®Œæ•´çš„è®­ç»ƒæŸå¤±**ï¼š

```python
class CombinedLoss(nn.Module):
    def __init__(self, config):
        super().__init__()

        # ä¸»åˆ†ç±»æŸå¤±
        self.ce_loss = PowersetBCEWithLogitsLoss()

        # è’¸é¦æŸå¤±ï¼ˆå‰ªææ—¶ä½¿ç”¨ï¼‰
        if config.get('use_distillation', False):
            self.distill_loss = DistillationLoss(
                temperature=config.get('distill_temp', 2.0),
                alpha=config.get('distill_alpha', 0.7)
            )

        # æ—¶åºä¸€è‡´æ€§æŸå¤±
        if config.get('use_temporal_consistency', True):
            self.temporal_loss = TemporalConsistencyLoss(
                weight=config.get('temporal_weight', 0.1)
            )

        # æƒé‡é…ç½®
        self.weights = {
            'ce': config.get('ce_weight', 1.0),
            'distill': config.get('distill_weight', 1.0),
            'temporal': config.get('temporal_weight', 0.1)
        }

    def forward(self, logits, targets, teacher_logits=None):
        """
        Args:
            logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º
            targets: çœŸå®æ ‡ç­¾
            teacher_logits: æ•™å¸ˆæ¨¡å‹è¾“å‡ºï¼ˆå¯é€‰ï¼‰
        Returns:
            total_loss: æ€»æŸå¤±
            loss_dict: å„æŸå¤±ç»„ä»¶
        """

        loss_dict = {}

        # ä¸»åˆ†ç±»æŸå¤±
        ce_loss = self.ce_loss(logits, targets)
        total_loss = self.weights['ce'] * ce_loss
        loss_dict['ce_loss'] = ce_loss.item()

        # è’¸é¦æŸå¤±
        if hasattr(self, 'distill_loss') and teacher_logits is not None:
            distill_loss, distill_components = self.distill_loss(
                logits, teacher_logits, targets
            )
            total_loss += self.weights['distill'] * distill_loss
            loss_dict.update(distill_components)

        # æ—¶åºä¸€è‡´æ€§æŸå¤±
        if hasattr(self, 'temporal_loss'):
            temp_loss = self.temporal_loss(logits)
            total_loss += self.weights['temporal'] * temp_loss
            loss_dict['temporal_loss'] = temp_loss.item()

        loss_dict['total_loss'] = total_loss.item()

        return total_loss, loss_dict
```

**æŸå¤±å‡½æ•°è°ƒä¼˜ç­–ç•¥**ï¼š
```python
def adaptive_loss_weighting(loss_history, current_epoch):
    """è‡ªé€‚åº”æŸå¤±æƒé‡è°ƒæ•´"""

    # åŸºäºè®­ç»ƒç¨³å®šæ€§è°ƒæ•´æƒé‡
    if len(loss_history) > 10:
        loss_variance = np.var(loss_history[-10:])

        if loss_variance > 0.1:  # æŸå¤±ä¸ç¨³å®š
            # å¢åŠ ä¸»æŸå¤±æƒé‡ï¼Œå‡å°‘æ­£åˆ™åŒ–
            weights = {'ce': 1.2, 'temporal': 0.05}
        else:
            # æ­£å¸¸æƒé‡
            weights = {'ce': 1.0, 'temporal': 0.1}

        # åŸºäºepochè°ƒæ•´è’¸é¦æƒé‡
        if current_epoch < 20:
            weights['distill'] = 0.3  # æ—©æœŸé‡ç‚¹å­¦ä¹ æ•™å¸ˆçŸ¥è¯†
        else:
            weights['distill'] = 0.7  # åæœŸå¢å¼ºå­¦ç”Ÿè‡ªä¸»å­¦ä¹ 

    return weights
```

### 4. èšç±»ç®—æ³•è¯¦è§£

èšç±»æ˜¯è¯´è¯äººåˆ†ç¦»ç³»ç»Ÿçš„æœ€åä¹Ÿæ˜¯æœ€å…³é”®çš„æ­¥éª¤ï¼Œå®ƒå°†æå–çš„è¯´è¯äººåµŒå…¥å‘é‡å½’ç±»ä¸ºä¸åŒçš„è¯´è¯äººã€‚DiariZenæ”¯æŒä¸¤ç§ä¸»è¦çš„èšç±»ç®—æ³•ï¼Œæ¯ç§éƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿å’Œé€‚ç”¨åœºæ™¯ã€‚

#### ğŸ“Š ç®—æ³•å¯¹æ¯”è¡¨

| ç‰¹æ€§ | VBxèšç±» | å±‚æ¬¡èšç±» |
|------|---------|----------|
| **è¯´è¯äººæ•°é‡** | è‡ªåŠ¨ç¡®å®š | éœ€è¦æŒ‡å®šèŒƒå›´ |
| **æ—¶åºå»ºæ¨¡** | âœ… HMMå»ºæ¨¡ | âŒ ç‹¬ç«‹å¤„ç† |
| **è®¡ç®—å¤æ‚åº¦** | é«˜ O(TÃ—KÂ²Ã—I) | ä¸­ O(NÂ²log N) |
| **å‡†ç¡®æ€§** | é«˜ | ä¸­ç­‰ |
| **é²æ£’æ€§** | å¼º | ä¸­ç­‰ |
| **å®æ—¶æ€§** | æ…¢ | å¿« |

#### ğŸ§  VBxå˜åˆ†è´å¶æ–¯èšç±»

**ç®—æ³•åŸç†**

VBxï¼ˆVariational Bayes x-vectorsï¼‰æ˜¯ä¸€ç§åŸºäº**å˜åˆ†è´å¶æ–¯æ¨ç†**çš„èšç±»ç®—æ³•ï¼Œä¸“ä¸ºè¯´è¯äººåˆ†ç¦»ä»»åŠ¡è®¾è®¡ã€‚å®ƒå°†èšç±»é—®é¢˜å»ºæ¨¡ä¸ºä¸€ä¸ª**ç”Ÿæˆæ¨¡å‹**ï¼Œå¹¶ä½¿ç”¨å˜åˆ†æ¨ç†æ±‚è§£ã€‚

**æ•°å­¦æ¨¡å‹**ï¼šVBxå‡è®¾æ¯ä¸ªè¯´è¯äººåµŒå…¥å‘é‡ç”±ä¸€ä¸ª**æ··åˆé«˜æ–¯æ¨¡å‹**ç”Ÿæˆï¼Œå¹¶è€ƒè™‘è¯´è¯äººè½¬æ¢çš„æ—¶åºè¿ç»­æ€§ã€‚

**å…³é”®åˆ›æ–°**ï¼š
1. **HMMæ—¶åºå»ºæ¨¡**ï¼šè€ƒè™‘è¯´è¯äººè½¬æ¢çš„æ—¶åºè¿ç»­æ€§
2. **å˜åˆ†æ¨ç†**ï¼šå¤„ç†æ¨¡å‹é€‰æ‹©å’Œå‚æ•°ä¸ç¡®å®šæ€§
3. **è‡ªåŠ¨æ¨¡å‹é€‰æ‹©**ï¼šæ— éœ€é¢„å…ˆæŒ‡å®šè¯´è¯äººæ•°é‡

**å‚æ•°è¯¦è§£**ï¼š
```python
vbx_config = {
    "method": "VBxClustering",
    "ahc_threshold": 0.6,    # AHCåˆå§‹åŒ–é˜ˆå€¼
    "Fa": 0.07,              # ç»Ÿè®¡é‡ç¼©æ”¾å› å­ï¼ˆ0.05-0.1ï¼‰
    "Fb": 0.8,               # è¯´è¯äººæ­£åˆ™åŒ–ç³»æ•°ï¼ˆ0.5-1.0ï¼‰
    "lda_dim": 128,          # LDAé™ç»´ç»´åº¦
    "max_iters": 20          # æœ€å¤§è¿­ä»£æ¬¡æ•°
}
```

**å‚æ•°å«ä¹‰**ï¼š
- **Fa**: æ§åˆ¶èšç±»ç´§å¯†åº¦ï¼Œè¾ƒå°å€¼(0.05)æ£€æµ‹æ›´å¤šè¯´è¯äººï¼Œè¾ƒå¤§å€¼(0.1)æ›´ä¿å®ˆ
- **Fb**: æ§åˆ¶æœ€ç»ˆè¯´è¯äººæ•°é‡ï¼Œè¾ƒå°å€¼å€¾å‘æ›´å¤šè¯´è¯äººï¼Œè¾ƒå¤§å€¼æ›´å°‘
- **ahc_threshold**: åˆå§‹èšç±»çš„åˆå¹¶é˜ˆå€¼ï¼Œå½±å“VBxçš„èµ·å§‹ç‚¹

#### ğŸŒ³ å±‚æ¬¡èšç±» (AgglomerativeClustering)

**ç®—æ³•åŸç†**

å±‚æ¬¡èšç±»æ˜¯ä¸€ç§ç»å…¸çš„**è‡ªåº•å‘ä¸Š**èšç±»ç®—æ³•ï¼Œé€šè¿‡é€æ­¥åˆå¹¶æœ€ç›¸ä¼¼çš„èšç±»æ¥æ„å»ºå±‚æ¬¡ç»“æ„ã€‚åŸºäºè¯´è¯äººåµŒå…¥å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œèšç±»ã€‚

**ç®—æ³•æµç¨‹**ï¼š
1. åˆå§‹åŒ–ï¼šæ¯ä¸ªåµŒå…¥å‘é‡ä¸ºä¸€ä¸ªèšç±»
2. è®¡ç®—ï¼šæ‰€æœ‰èšç±»å¯¹ä¹‹é—´çš„è·ç¦»
3. åˆå¹¶ï¼šè·ç¦»æœ€è¿‘çš„ä¸¤ä¸ªèšç±»
4. é‡å¤ï¼šç›´åˆ°è¾¾åˆ°åœæ­¢æ¡ä»¶

**å‚æ•°è¯¦è§£**ï¼š
```python
ahc_config = {
    "method": "AgglomerativeClustering",
    "ahc_threshold": 0.70,        # åˆå¹¶é˜ˆå€¼ï¼ˆ0.5-0.9ï¼‰
    "min_cluster_size": 13,       # æœ€å°èšç±»å¤§å°
    "linkage_method": "centroid", # è¿æ¥æ–¹æ³•
    "min_speakers": 1,            # æœ€å°‘è¯´è¯äººæ•°
    "max_speakers": 20            # æœ€å¤šè¯´è¯äººæ•°
}
```

**å‚æ•°å«ä¹‰**ï¼š
- **ahc_threshold**: åˆå¹¶é˜ˆå€¼ï¼Œ0.7ä¸ºå¹³è¡¡è®¾ç½®ï¼Œ0.5æ¿€è¿›åˆå¹¶ï¼Œ0.9ä¿å®ˆåˆå¹¶
- **min_cluster_size**: æœ€å°èšç±»å¤§å°ï¼ŒåŸºäºéŸ³é¢‘æ—¶é•¿è°ƒæ•´ï¼ˆçŸ­éŸ³é¢‘ç”¨5-10ï¼Œé•¿éŸ³é¢‘ç”¨20-30ï¼‰
- **linkage_method**: "centroid"ä½¿ç”¨ä¸­å¿ƒç‚¹è·ç¦»ï¼ˆæ¨èï¼‰ï¼Œ"complete"äº§ç”Ÿç´§å¯†èšç±»

#### âš–ï¸ ç®—æ³•é€‰æ‹©æŒ‡å—

**é€‰æ‹©å†³ç­–æ ‘**ï¼š
```python
def choose_algorithm(audio_duration, expected_speakers, accuracy_priority, real_time_required):
    if real_time_required or audio_duration < 60:
        return "AgglomerativeClustering"
    elif accuracy_priority and expected_speakers > 4:
        return "VBxClustering"
    else:
        return "VBxClustering" if audio_duration > 300 else "AgglomerativeClustering"
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
| åœºæ™¯ | æ¨èç®—æ³• | é¢„æœŸDER | å¤„ç†æ—¶é—´ |
|------|----------|---------|----------|
| çŸ­å¯¹è¯(<1åˆ†é’Ÿ, 2-3äºº) | AHC | 8-12% | <1ç§’ |
| ä¼šè®®(5-30åˆ†é’Ÿ, 4-8äºº) | VBx | 12-18% | 10-60ç§’ |
| é•¿ä¼šè®®(>30åˆ†é’Ÿ, >6äºº) | VBx | 15-25% | 1-5åˆ†é’Ÿ |

#### ğŸ¯ åœºæ™¯åŒ–å‚æ•°ä¼˜åŒ–

**ä¼šè®®åœºæ™¯é…ç½®**ï¼š
```python
meeting_config = {
    "method": "VBxClustering",
    "Fa": 0.05,              # æ•æ„Ÿæ£€æµ‹ï¼Œé€‚åˆå¤šè¯´è¯äºº
    "Fb": 0.8,               # å¹³è¡¡è®¾ç½®
    "ahc_threshold": 0.6,    # è¾ƒä½é˜ˆå€¼ï¼Œåˆå§‹æ›´å¤šèšç±»
    "lda_dim": 128,
    "max_iters": 20
}
```

**å¯¹è¯åœºæ™¯é…ç½®**ï¼š
```python
dialog_config = {
    "method": "AgglomerativeClustering",
    "ahc_threshold": 0.75,   # è¾ƒé«˜é˜ˆå€¼ï¼Œé¿å…è¿‡åˆ†å‰²
    "min_cluster_size": 8,   # è¾ƒå°æœ€å°èšç±»å¤§å°
}
```

**æ’­å®¢åœºæ™¯é…ç½®**ï¼š
```python
podcast_config = {
    "method": "VBxClustering", 
    "Fa": 0.08,              # é€‚ä¸­æ•æ„Ÿåº¦
    "Fb": 0.85,              # å€¾å‘è¾ƒå°‘è¯´è¯äºº
    "ahc_threshold": 0.7,
    "max_iters": 25          # å¢åŠ è¿­ä»£æ¬¡æ•°æé«˜ç²¾åº¦
}
```

#### ğŸ› ï¸ é«˜çº§ä¼˜åŒ–æŠ€å·§

**1. è‡ªé€‚åº”å‚æ•°è°ƒæ•´**ï¼š
```python
def adaptive_clustering_config(audio_duration, activity_ratio):
    """åŸºäºéŸ³é¢‘ç‰¹æ€§è‡ªé€‚åº”è°ƒæ•´å‚æ•°"""
    base_config = {
        "method": "VBxClustering",
        "Fa": 0.07,
        "Fb": 0.8,
        "ahc_threshold": 0.6
    }
    
    # æ ¹æ®éŸ³é¢‘æ—¶é•¿è°ƒæ•´
    if audio_duration > 1800:  # >30åˆ†é’Ÿ
        base_config["Fa"] *= 0.8  # é™ä½æ•æ„Ÿåº¦
        base_config["max_iters"] = 30
    
    # æ ¹æ®è¯­éŸ³æ´»åŠ¨æ¯”ä¾‹è°ƒæ•´
    if activity_ratio > 0.8:  # é«˜æ´»åŠ¨åº¦
        base_config["Fb"] *= 0.9  # å€¾å‘æ›´å¤šè¯´è¯äºº
        
    return base_config
```

**2. è´¨é‡ç›‘æ§ä¸åŠ¨æ€è°ƒæ•´**ï¼š
```python
def monitor_clustering_quality(embeddings, clusters):
    """ç›‘æ§èšç±»è´¨é‡å¹¶ç»™å‡ºè°ƒæ•´å»ºè®®"""
    from sklearn.metrics import silhouette_score
    
    if len(set(clusters)) < 2:
        return {"quality": "poor", "suggestion": "é™ä½é˜ˆå€¼ï¼Œå¢åŠ Fa"}
    
    sil_score = silhouette_score(embeddings, clusters)
    
    if sil_score < 0.3:
        return {"quality": "poor", "suggestion": "è°ƒæ•´Faå’ŒFbå‚æ•°"}
    elif sil_score < 0.5:
        return {"quality": "fair", "suggestion": "å¯å°è¯•å¾®è°ƒå‚æ•°"}
    else:
        return {"quality": "good", "suggestion": "å‚æ•°è®¾ç½®åˆç†"}
```

**3. æ··åˆç­–ç•¥**ï¼š
```python
class HybridClustering:
    """ç»“åˆAHCå’ŒVBxä¼˜åŠ¿çš„æ··åˆç­–ç•¥"""
    
    def __call__(self, embeddings, segmentations, **kwargs):
        # å¿«é€Ÿé¢„èšç±»
        ahc_clusters = self.ahc_clustering(embeddings)
        
        # è¯„ä¼°è´¨é‡
        quality = self.evaluate_quality(embeddings, ahc_clusters)
        
        # æ ¹æ®è´¨é‡å†³å®šæ˜¯å¦VBxä¼˜åŒ–
        if quality < 0.8:
            return self.vbx_refinement(embeddings, ahc_clusters)
        else:
            return ahc_clusters
```

#### ğŸ“Š å®æˆ˜è°ƒä¼˜æŒ‡å—

**é—®é¢˜è¯Šæ–­ä¸è§£å†³**ï¼š

| é—®é¢˜ç°è±¡ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|----------|----------|----------|
| è¯´è¯äººè¿‡å¤š | Faå€¼è¿‡å° | å¢åŠ Faåˆ°0.08-0.1 |
| è¯´è¯äººè¿‡å°‘ | Fbå€¼è¿‡å¤§ | é™ä½Fbåˆ°0.6-0.7 |
| åˆ†å‰²è¿‡ç»† | thresholdè¿‡ä½ | æé«˜åˆ°0.75-0.8 |
| åˆå¹¶è¿‡åº¦ | thresholdè¿‡é«˜ | é™ä½åˆ°0.5-0.6 |

**å‚æ•°è°ƒä¼˜æ­¥éª¤**ï¼š
1. **åŸºçº¿æµ‹è¯•**ï¼šä½¿ç”¨é»˜è®¤å‚æ•°è·å¾—åŸºç¡€ç»“æœ
2. **å•å‚æ•°è°ƒä¼˜**ï¼šé€ä¸€è°ƒæ•´å…³é”®å‚æ•°è§‚å¯Ÿæ•ˆæœ
3. **ç½‘æ ¼æœç´¢**ï¼šåœ¨æœ‰å¸Œæœ›çš„å‚æ•°èŒƒå›´å†…ç²¾ç»†æœç´¢
4. **äº¤å‰éªŒè¯**ï¼šä½¿ç”¨å¤šä¸ªéŸ³é¢‘æ ·æœ¬éªŒè¯å‚æ•°ç¨³å®šæ€§

**è‡ªåŠ¨è°ƒä¼˜ä»£ç ç¤ºä¾‹**ï¼š
```python
def auto_tune_parameters(validation_audios, target_speakers):
    """è‡ªåŠ¨è°ƒä¼˜èšç±»å‚æ•°"""
    best_params = {}
    best_score = float('inf')
    
    param_ranges = {
        'Fa': [0.03, 0.05, 0.07, 0.1, 0.12],
        'Fb': [0.6, 0.7, 0.8, 0.9, 1.0],
        'ahc_threshold': [0.5, 0.6, 0.7, 0.8]
    }
    
    for fa in param_ranges['Fa']:
        for fb in param_ranges['Fb']:
            for threshold in param_ranges['ahc_threshold']:
                config = {
                    'Fa': fa, 'Fb': fb, 
                    'ahc_threshold': threshold
                }
                
                total_error = 0
                for audio, true_speakers in zip(validation_audios, target_speakers):
                    predicted = run_clustering(audio, config)
                    error = abs(len(predicted) - true_speakers)
                    total_error += error
                
                if total_error < best_score:
                    best_score = total_error
                    best_params = config
    
    return best_params, best_score
```

---

## å¿«é€Ÿå¼€å§‹

### æœ€ç®€å•ç”¨æ³•

```python
from diarizen.pipelines.inference import DiariZenPipeline

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
pipeline = DiariZenPipeline.from_pretrained("BUT-FIT/diarizen-wavlm-large-s80-md")

# 2. å¤„ç†éŸ³é¢‘æ–‡ä»¶
results = pipeline('./example/EN2002a_30s.wav')

# 3. æŸ¥çœ‹ç»“æœ
for turn, _, speaker in results.itertracks(yield_label=True):
    print(f"æ—¶é—´: {turn.start:.1f}-{turn.end:.1f}ç§’, è¯´è¯äºº: {speaker}")
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```
æ—¶é—´: 0.0-2.7ç§’, è¯´è¯äºº: 0
æ—¶é—´: 0.8-13.6ç§’, è¯´è¯äºº: 3  
æ—¶é—´: 5.8-6.4ç§’, è¯´è¯äºº: 0
...
```

### ä¿å­˜RTTMæ–‡ä»¶

```python
# è‡ªåŠ¨ä¿å­˜RTTMæ ¼å¼ç»“æœ
pipeline = DiariZenPipeline.from_pretrained(
    "BUT-FIT/diarizen-wavlm-large-s80-md",
    rttm_out_dir='./output'  # æŒ‡å®šè¾“å‡ºç›®å½•
)

# å¤„ç†æ—¶æŒ‡å®šä¼šè¯åç§°
results = pipeline('./audio.wav', sess_name='meeting_001')
# å°†è‡ªåŠ¨ç”Ÿæˆ ./output/meeting_001.rttm
```

### æ‰¹é‡å¤„ç†è„šæœ¬

```python
import os
from pathlib import Path
from diarizen.pipelines.inference import DiariZenPipeline

def batch_diarization(audio_dir, output_dir):
    """æ‰¹é‡å¤„ç†éŸ³é¢‘æ–‡ä»¶"""
    
    # åŠ è½½æ¨¡å‹
    pipeline = DiariZenPipeline.from_pretrained(
        "BUT-FIT/diarizen-wavlm-large-s80-md",
        rttm_out_dir=output_dir
    )
    
    # éå†éŸ³é¢‘æ–‡ä»¶
    audio_files = list(Path(audio_dir).glob("*.wav"))
    
    for audio_file in audio_files:
        print(f"æ­£åœ¨å¤„ç†: {audio_file.name}")
        
        try:
            # æ‰§è¡Œè¯´è¯äººåˆ†ç¦»
            results = pipeline(str(audio_file), sess_name=audio_file.stem)
            
            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            speakers = set()
            total_duration = 0
            for turn, _, speaker in results.itertracks(yield_label=True):
                speakers.add(speaker)
                total_duration += turn.duration
                
            print(f"  - æ£€æµ‹åˆ° {len(speakers)} ä¸ªè¯´è¯äºº")
            print(f"  - æ€»è¯´è¯æ—¶é•¿: {total_duration:.1f}ç§’")
            
        except Exception as e:
            print(f"  - å¤„ç†å¤±è´¥: {e}")
    
    print("æ‰¹é‡å¤„ç†å®Œæˆï¼")

# ä½¿ç”¨ç¤ºä¾‹
batch_diarization('./audio_files', './diarization_results')
```

---

## APIè¯¦ç»†è¯´æ˜

### DiariZenPipelineç±»

#### åˆå§‹åŒ–å‚æ•°
```python
class DiariZenPipeline:
    def __init__(
        self,
        diarizen_hub: Path,              # æ¨¡å‹æ–‡ä»¶è·¯å¾„
        embedding_model: str,            # åµŒå…¥æ¨¡å‹è·¯å¾„
        config_parse: Dict = None,       # é…ç½®è¦†ç›–
        rttm_out_dir: str = None        # RTTMè¾“å‡ºç›®å½•
    )
```

#### from_pretrainedæ–¹æ³•
```python
@classmethod
def from_pretrained(
    cls,
    repo_id: str,                       # HuggingFaceæ¨¡å‹ID
    cache_dir: str = None,              # ç¼“å­˜ç›®å½•
    rttm_out_dir: str = None           # RTTMè¾“å‡ºç›®å½•
) -> "DiariZenPipeline"
```

**å¯ç”¨æ¨¡å‹**ï¼š
- `"BUT-FIT/diarizen-wavlm-base-s80-md"` - åŸºç¡€ç‰ˆæœ¬ï¼ˆè¾ƒå¿«ï¼‰
- `"BUT-FIT/diarizen-wavlm-large-s80-md"` - å¤§å‹ç‰ˆæœ¬ï¼ˆæ›´å‡†ç¡®ï¼‰

#### __call__æ–¹æ³•è¯¦è§£

**æ ¸å¿ƒæ¨ç†æ¥å£**ï¼š
```python
def __call__(
    self,
    in_wav: Union[str, Path, ProtocolFile],
    sess_name: str = None,
    num_speakers: int = None,
    min_speakers: int = None,
    max_speakers: int = None,
    return_embeddings: bool = False
) -> Union[Annotation, Tuple[Annotation, np.ndarray]]
```

**å‚æ•°è¯´æ˜**ï¼š
- **in_wav**: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„æˆ–ProtocolFileå¯¹è±¡
- **sess_name**: ä¼šè¯åç§°ï¼Œç”¨äºRTTMè¾“å‡ºæ–‡ä»¶å
- **num_speakers**: å¼ºåˆ¶æŒ‡å®šè¯´è¯äººæ•°é‡ï¼ˆå¯é€‰ï¼‰
- **min_speakers/max_speakers**: è¯´è¯äººæ•°é‡èŒƒå›´çº¦æŸ
- **return_embeddings**: æ˜¯å¦è¿”å›è¯´è¯äººåµŒå…¥å‘é‡

**æ¨ç†æµç¨‹**ï¼š
```python
def __call__(self, in_wav, **kwargs):
    """
    å®Œæ•´çš„æ¨ç†æµç¨‹ï¼š
    1. éŸ³é¢‘é¢„å¤„ç†
    2. æ»‘åŠ¨çª—å£åˆ†å‰²
    3. æ‰¹å¤„ç†æ¨ç†
    4. åµŒå…¥æå–
    5. èšç±»åˆ†æ
    6. åå¤„ç†ä¼˜åŒ–
    7. ç»“æœæ ¼å¼åŒ–
    """

    # 1. éŸ³é¢‘é¢„å¤„ç†
    waveform = self._preprocess_audio(in_wav)

    # 2. åˆ†å‰²æ¨ç†
    segmentations = self._sliding_window_inference(waveform)

    # 3. åµŒå…¥æå–
    embeddings, segments = self._extract_embeddings(segmentations, waveform)

    # 4. èšç±»åˆ†æ
    clusters = self._perform_clustering(embeddings, **kwargs)

    # 5. ç»“æœæ•´åˆ
    annotation = self._create_annotation(segments, clusters)

    # 6. åå¤„ç†
    annotation = self._post_process(annotation)

    # 7. å¯é€‰ï¼šä¿å­˜RTTM
    if self.rttm_out_dir:
        self._save_rttm(annotation, kwargs.get('sess_name', 'unknown'))

    return annotation
```

**ç§æœ‰æ–¹æ³•è¯¦è§£**ï¼š

```python
def _preprocess_audio(self, audio_input):
    """éŸ³é¢‘é¢„å¤„ç†"""
    # åŠ è½½éŸ³é¢‘
    if isinstance(audio_input, str):
        waveform, sample_rate = torchaudio.load(audio_input)
    else:
        # ProtocolFileå¤„ç†
        waveform, sample_rate = self._load_from_protocol(audio_input)

    # é‡é‡‡æ ·åˆ°16kHz
    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(sample_rate, 16000)
        waveform = resampler(waveform)

    # è½¬æ¢ä¸ºå•å£°é“
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    # é•¿åº¦æ£€æŸ¥
    min_length = self.segmentation.model.receptive_field_size()
    if waveform.shape[1] < min_length:
        # å¡«å……çŸ­éŸ³é¢‘
        padding = torch.zeros(1, min_length - waveform.shape[1])
        waveform = torch.cat([waveform, padding], dim=1)

    return waveform

def _sliding_window_inference(self, waveform):
    """æ»‘åŠ¨çª—å£åˆ†å‰²æ¨ç†"""

    chunk_duration = self.segmentation.model.chunk_size  # 8ç§’
    step_ratio = self.segmentation_step  # 0.1

    chunk_samples = int(chunk_duration * self.segmentation.model.sample_rate)
    step_samples = int(chunk_samples * step_ratio)

    segmentations = []
    start_sample = 0

    while start_sample < waveform.shape[1]:
        end_sample = min(start_sample + chunk_samples, waveform.shape[1])

        # æå–éŸ³é¢‘å—
        chunk = waveform[:, start_sample:end_sample]

        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        if chunk.shape[1] < chunk_samples:
            padding_length = chunk_samples - chunk.shape[1]
            padding = torch.zeros(1, padding_length)
            chunk = torch.cat([chunk, padding], dim=1)

        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            outputs = self.segmentation.model(chunk.unsqueeze(0).to(self.device))
            probs = torch.sigmoid(outputs[0])  # [T, num_classes]

        # è§£ç ä¸ºè¯´è¯äººæ´»åŠ¨
        speaker_activity = self._decode_powerset(probs)

        segmentation = {
            'start_time': start_sample / self.segmentation.model.sample_rate,
            'end_time': end_sample / self.segmentation.model.sample_rate,
            'speaker_activity': speaker_activity,  # [T, max_speakers]
            'probabilities': probs.cpu().numpy()   # ä¿å­˜åŸå§‹æ¦‚ç‡
        }

        segmentations.append(segmentation)
        start_sample += step_samples

    return segmentations

def _extract_embeddings(self, segmentations, waveform):
    """æå–è¯´è¯äººåµŒå…¥"""

    embeddings = []
    segments_for_embedding = []

    # æ£€æµ‹æ‰€æœ‰æœ‰å£°æ®µ
    for seg_idx, seg in enumerate(segmentations):
        speaker_activity = seg['speaker_activity']

        for speaker_idx in range(speaker_activity.shape[1]):
            activity = speaker_activity[:, speaker_idx]

            # æ‰¾åˆ°è¿ç»­çš„æ´»è·ƒæ®µ
            active_frames = activity > 0.5
            if active_frames.sum() == 0:
                continue

            # åˆ†å‰²ä¸ºè¿ç»­æ®µ
            active_segments = self._find_continuous_segments(active_frames)

            for start_frame, end_frame in active_segments:
                # è½¬æ¢ä¸ºæ—¶é—´
                start_time = seg['start_time'] + start_frame * self.segmentation.model.get_rf_info()[2]
                end_time = seg['start_time'] + (end_frame + 1) * self.segmentation.model.get_rf_info()[2]

                # è·³è¿‡å¤ªçŸ­çš„æ®µ
                if end_time - start_time < 0.5:  # è‡³å°‘0.5ç§’
                    continue

                segments_for_embedding.append({
                    'start': start_time,
                    'end': end_time,
                    'speaker_idx': speaker_idx,
                    'segmentation_idx': seg_idx
                })

    # æ‰¹å¤„ç†åµŒå…¥æå–
    if segments_for_embedding:
        # å‡†å¤‡æ‰¹æ¬¡
        batch_segments = []
        for seg in segments_for_embedding:
            batch_segments.append({
                'waveform': waveform,
                'start': seg['start'],
                'end': seg['end']
            })

        # æå–åµŒå…¥
        batch_embeddings = self.embedding(batch_segments)

        for seg, emb in zip(segments_for_embedding, batch_embeddings):
            embeddings.append({
                'embedding': emb,
                'segment': seg
            })

    return embeddings, segments_for_embedding

def _perform_clustering(self, embeddings, **kwargs):
    """æ‰§è¡Œèšç±»åˆ†æ"""

    if not embeddings:
        return []

    # æå–åµŒå…¥å‘é‡
    embedding_vectors = torch.stack([emb['embedding'] for emb in embeddings])

    # è·å–èšç±»å‚æ•°
    clustering_params = self._get_clustering_params(**kwargs)

    # æ‰§è¡Œèšç±»
    if self.clustering == "AgglomerativeClustering":
        clusters = self._agglomerative_clustering(embedding_vectors, **clustering_params)
    elif self.clustering == "VBxClustering":
        clusters = self._vbx_clustering(embedding_vectors, **clustering_params)
    else:
        raise ValueError(f"Unsupported clustering method: {self.clustering}")

    return clusters

def _create_annotation(self, segments, clusters):
    """åˆ›å»ºpyannote.Annotationå¯¹è±¡"""

    from pyannote.core import Annotation, Segment

    annotation = Annotation()

    # ä¸ºæ¯ä¸ªèšç±»åˆ†é…è¯´è¯äººæ ‡ç­¾
    cluster_to_speaker = {}
    speaker_counter = 0

    for segment_info, cluster_id in zip(segments, clusters):
        if cluster_id not in cluster_to_speaker:
            cluster_to_speaker[cluster_id] = f"speaker_{speaker_counter:02d}"
            speaker_counter += 1

        speaker_label = cluster_to_speaker[cluster_id]

        # æ·»åŠ åˆ°æ ‡æ³¨
        segment = Segment(segment_info['start'], segment_info['end'])
        annotation[segment, '_'] = speaker_label

    return annotation
```

### é…ç½®å‚æ•°è¯¦è§£

#### æ¨ç†é…ç½®
```python
inference_config = {
    "seg_duration": 16,                 # åˆ†æ®µé•¿åº¦ï¼ˆç§’ï¼‰
    "segmentation_step": 0.1,           # æ»‘åŠ¨çª—å£æ­¥é•¿æ¯”ä¾‹
    "batch_size": 32,                   # æ‰¹å¤„ç†å¤§å°
    "apply_median_filtering": True      # æ˜¯å¦åº”ç”¨ä¸­å€¼æ»¤æ³¢
}
```

#### èšç±»é…ç½®

**VBxèšç±»**ï¼š
```python
vbx_config = {
    "method": "VBxClustering",
    "min_speakers": 1,                  # æœ€å°‘è¯´è¯äººæ•°
    "max_speakers": 20,                 # æœ€å¤šè¯´è¯äººæ•°
    "ahc_criterion": "distance",        # AHCå‡†åˆ™
    "ahc_threshold": 0.6,               # AHCé˜ˆå€¼
    "Fa": 0.07,                        # ç»Ÿè®¡é‡ç¼©æ”¾
    "Fb": 0.8,                         # è¯´è¯äººæ­£åˆ™åŒ–
    "lda_dim": 128,                    # LDAé™ç»´ç»´åº¦
    "max_iters": 20                    # æœ€å¤§è¿­ä»£æ¬¡æ•°
}
```

**å±‚æ¬¡èšç±»**ï¼š
```python
ahc_config = {
    "method": "AgglomerativeClustering",
    "min_speakers": 1,                  # æœ€å°‘è¯´è¯äººæ•°
    "max_speakers": 20,                 # æœ€å¤šè¯´è¯äººæ•°
    "ahc_threshold": 0.70,              # åˆå¹¶é˜ˆå€¼
    "min_cluster_size": 13              # æœ€å°èšç±»å¤§å°
}
```

### ç»“æœå¤„ç†

#### Annotationå¯¹è±¡æ–¹æ³•
```python
# éå†æ‰€æœ‰è¯´è¯æ®µ
for segment, track, label in annotation.itertracks(yield_label=True):
    start_time = segment.start          # å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    end_time = segment.end             # ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    duration = segment.duration        # æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
    speaker = label                    # è¯´è¯äººæ ‡ç­¾

# è·å–ç‰¹å®šæ—¶é—´ç‚¹çš„è¯´è¯äºº
speaker_at_5s = annotation.argmax(Segment(5.0, 5.0))

# è®¡ç®—é‡å è¯´è¯æ¯”ä¾‹
overlap_ratio = annotation.get_overlap().duration() / annotation.get_timeline().duration()

# å¯¼å‡ºä¸ºRTTMæ ¼å¼
rttm_content = annotation.to_rttm()
```

#### RTTMæ ¼å¼è¯´æ˜
```
SPEAKER filename 1 start_time duration <NA> <NA> speaker_id <NA> <NA>
```

ç¤ºä¾‹ï¼š
```
SPEAKER meeting_001 1 0.00 2.70 <NA> <NA> 0 <NA> <NA>
SPEAKER meeting_001 1 2.70 1.80 <NA> <NA> 1 <NA> <NA>
SPEAKER meeting_001 1 4.50 3.20 <NA> <NA> 0 <NA> <NA>
```

---

## è®­ç»ƒæµç¨‹

### æ•°æ®å‡†å¤‡

#### ç›®å½•ç»“æ„
```
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ wav.scp          # éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
â”‚   â”œâ”€â”€ rttm             # è¯´è¯äººæ ‡æ³¨
â”‚   â””â”€â”€ all.uem          # è¯„ä¼°æ®µæ ‡è®°
â”œâ”€â”€ dev/
â”‚   â”œâ”€â”€ wav.scp
â”‚   â”œâ”€â”€ rttm
â”‚   â””â”€â”€ all.uem
â””â”€â”€ test/
    â”œâ”€â”€ wav.scp
    â”œâ”€â”€ rttm
    â””â”€â”€ all.uem
```

#### æ–‡ä»¶æ ¼å¼

**wav.scpæ ¼å¼**ï¼š
```
session_id1 /path/to/audio1.wav
session_id2 /path/to/audio2.wav
session_id3 /path/to/audio3.wav
```

**RTTMæ ¼å¼**ï¼š
```
SPEAKER session_id1 1 0.00 2.50 <NA> <NA> spk1 <NA> <NA>
SPEAKER session_id1 1 2.50 1.80 <NA> <NA> spk2 <NA> <NA>
SPEAKER session_id1 1 4.30 3.20 <NA> <NA> spk1 <NA> <NA>
```

**UEMæ ¼å¼**ï¼š
```
session_id1 1 0.00 30.00
session_id2 1 0.00 45.20
session_id3 1 0.00 28.70
```

### è®­ç»ƒé…ç½®è¯¦è§£

#### å®Œæ•´é…ç½®æ–‡ä»¶åˆ†æ

**æ ¸å¿ƒé…ç½®æ–‡ä»¶ (wavlm_updated_conformer.toml)**ï¼š

```toml
[meta]
save_dir = "exp/wavlm_conformer_exp"     # å®éªŒä¿å­˜ç›®å½•
seed = 3407                              # éšæœºç§å­ï¼Œç¡®ä¿å¯é‡å¤æ€§
experiment_name = "diarizen_wavlm_base"  # å®éªŒåç§°

[trainer]
path = "diarizen.trainer_dual_opt.Trainer"
[trainer.args]
# è®­ç»ƒæ§åˆ¶
max_epochs = 100                         # æœ€å¤§è®­ç»ƒè½®æ•°
max_steps = 50000                         # æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼ˆä¼˜å…ˆçº§é«˜äºepochsï¼‰
gradient_accumulation_steps = 1           # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
validation_interval = 1                   # æ¯éš”Nä¸ªepochéªŒè¯ä¸€æ¬¡

# æ—©åœå’Œä¿å­˜
max_patience = 10                         # æ—©åœè€å¿ƒå€¼
save_max_score = false                    # æ˜¯å¦ä¿å­˜æœ€é«˜åˆ†æ¨¡å‹
save_ckpt_interval = 1                    # æ¯éš”Nä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹
max_num_checkpoints = 50                  # æœ€å¤§ä¿å­˜æ£€æŸ¥ç‚¹æ•°é‡

# æ¨¡å‹æ§åˆ¶
freeze_wavlm = false                      # æ˜¯å¦å†»ç»“WavLMå‚æ•°
use_one_cycle_lr = false                  # æ˜¯å¦ä½¿ç”¨OneCycleå­¦ä¹ ç‡
lr_decay = false                          # æ˜¯å¦å¯ç”¨å­¦ä¹ ç‡è¡°å‡

# ç›‘æ§å’Œè°ƒè¯•
plot_norm = true                          # ç»˜åˆ¶æ¢¯åº¦èŒƒæ•°
plot_lr = true                            # ç»˜åˆ¶å­¦ä¹ ç‡æ›²çº¿
debug = false                             # è°ƒè¯•æ¨¡å¼
gradient_percentile = 90                  # æ¢¯åº¦ç™¾åˆ†ä½è£å‰ª
gradient_history_size = 1000              # æ¢¯åº¦å†å²å¤§å°

# ä¼˜åŒ–ç­–ç•¥
warmup_steps = 1000                       # é¢„çƒ­æ­¥æ•°
warmup_ratio = 0.1                        # é¢„çƒ­æ¯”ä¾‹
scheduler_name = "constant_schedule_with_warmup"

[optimizer_small]
path = "torch.optim.AdamW"
[optimizer_small.args]
lr = 2e-5                                 # WavLMå­¦ä¹ ç‡
weight_decay = 0.01                       # L2æ­£åˆ™åŒ–
betas = [0.9, 0.98]                       # Adam betaå‚æ•°
eps = 1e-8                                # æ•°å€¼ç¨³å®šæ€§

[optimizer_big]
path = "torch.optim.AdamW"
[optimizer_big.args]
lr = 1e-3                                 # Conformerå­¦ä¹ ç‡
weight_decay = 0.01                       # L2æ­£åˆ™åŒ–
betas = [0.9, 0.98]                       # Adam betaå‚æ•°
eps = 1e-8                                # æ•°å€¼ç¨³å®šæ€§

[model]
path = "diarizen.models.eend.model_wavlm_conformer.Model"
[model.args]
# WavLMé…ç½®
wavlm_src = "/path/to/WavLM-Base+.pt"    # WavLMæ¨¡å‹è·¯å¾„
wavlm_layer_num = 13                     # WavLMå±‚æ•°
wavlm_feat_dim = 768                     # WavLMç‰¹å¾ç»´åº¦

# Conformeré…ç½®
attention_in = 256                       # æ³¨æ„åŠ›ç»´åº¦
ffn_hidden = 1024                        # å‰é¦ˆç½‘ç»œéšè—å±‚
num_head = 4                             # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°
num_layer = 4                            # Conformerå±‚æ•°
kernel_size = 31                         # å·ç§¯æ ¸å¤§å°
dropout = 0.1                            # Dropoutæ¯”ä¾‹

# ä»»åŠ¡é…ç½®
max_speakers_per_chunk = 4               # æ¯å—æœ€å¤§è¯´è¯äººæ•°
max_speakers_per_frame = 2               # æ¯å¸§æœ€å¤§è¯´è¯äººæ•°
chunk_size = 8                           # è®­ç»ƒå—å¤§å°ï¼ˆç§’ï¼‰
sample_rate = 16000                      # é‡‡æ ·ç‡

# å…¶ä»–é…ç½®
use_posi = false                         # æ˜¯å¦ä½¿ç”¨ä½ç½®ç¼–ç 
output_activate_function = false         # è¾“å‡ºæ¿€æ´»å‡½æ•°
selected_channel = 0                     # é€‰æ‹©çš„éŸ³é¢‘é€šé“

[train_dataset]
path = "diarizen.dataset.DiarizationDataset"
[train_dataset.args]
# æ•°æ®æ–‡ä»¶
scp_file = "data/train/wav.scp"          # è®­ç»ƒéŸ³é¢‘åˆ—è¡¨
rttm_file = "data/train/rttm"            # è®­ç»ƒæ ‡æ³¨æ–‡ä»¶
uem_file = "data/train/all.uem"          # è¯„ä¼°æ®µæ ‡è®°

# æ•°æ®å¤„ç†
chunk_size = 8                           # æ•°æ®å—å¤§å°ï¼ˆç§’ï¼‰
chunk_shift = 6                          # æ•°æ®å—åç§»ï¼ˆç§’ï¼‰
sample_rate = 16000                      # é‡‡æ ·ç‡
num_workers = 4                          # æ•°æ®åŠ è½½è¿›ç¨‹æ•°

[train_dataset.dataloader]
batch_size = 16                          # æ‰¹å¤„ç†å¤§å°
drop_last = true                         # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´æ‰¹æ¬¡
pin_memory = true                        # å›ºå®šå†…å­˜ï¼Œæå‡GPUä¼ è¾“æ•ˆç‡
persistent_workers = true                # ä¿æŒworkerè¿›ç¨‹
prefetch_factor = 2                      # é¢„å–å› å­

[validate_dataset]
path = "diarizen.dataset.DiarizationDataset"
[validate_dataset.args]
scp_file = "data/dev/wav.scp"            # éªŒè¯éŸ³é¢‘åˆ—è¡¨
rttm_file = "data/dev/rttm"              # éªŒè¯æ ‡æ³¨æ–‡ä»¶
uem_file = "data/dev/all.uem"            # éªŒè¯UEM
chunk_size = 8                           # éªŒè¯å—å¤§å°
chunk_shift = 8                          # éªŒè¯å—åç§»ï¼ˆæ— é‡å ï¼‰
sample_rate = 16000                      # é‡‡æ ·ç‡
num_workers = 2                          # éªŒè¯æ—¶å‡å°‘è¿›ç¨‹æ•°

[validate_dataset.dataloader]
batch_size = 8                           # éªŒè¯æ‰¹å¤„ç†å¤§å°
drop_last = true                         # ä¸¢å¼ƒä¸å®Œæ•´æ‰¹æ¬¡
pin_memory = true                        # å›ºå®šå†…å­˜
persistent_workers = true                # ä¿æŒworkerè¿›ç¨‹
```

#### æ•°æ®é›†é…ç½®è¯¦è§£

**DiarizationDatasetç±»å®ç°**ï¼š

```python
class DiarizationDataset(torch.utils.data.Dataset):
    def __init__(self, scp_file, rttm_file, uem_file, chunk_size=8, chunk_shift=6,
                 sample_rate=16000, num_workers=4):
        super().__init__()

        self.chunk_size = chunk_size
        self.chunk_shift = chunk_shift
        self.sample_rate = sample_rate
        self.chunk_samples = chunk_size * sample_rate
        self.shift_samples = chunk_shift * sample_rate

        # åŠ è½½æ•°æ®æ–‡ä»¶
        self.audio_files = self._load_scp(scp_file)      # {session_id: audio_path}
        self.annotations = self._load_rttm(rttm_file)    # {session_id: annotation}
        self.uem_segments = self._load_uem(uem_file)     # {session_id: [start, end]}

        # ç”Ÿæˆè®­ç»ƒå—
        self.chunks = self._generate_chunks()

        # æ•°æ®å¢å¼º
        self.audio_augment = AudioAugmentation(sample_rate)
        self.spec_augment = SpecAugmentation()

    def _generate_chunks(self):
        """ç”Ÿæˆè®­ç»ƒæ•°æ®å—"""
        chunks = []

        for session_id, audio_path in self.audio_files.items():
            if session_id not in self.uem_segments:
                continue

            # è·å–éŸ³é¢‘æ—¶é•¿
            info = torchaudio.info(audio_path)
            duration = info.num_frames / info.sample_rate

            # è·å–æœ‰æ•ˆè¯„ä¼°æ®µ
            uem_start, uem_end = self.uem_segments[session_id]

            # ç”Ÿæˆæ»‘åŠ¨çª—å£å—
            start_time = uem_start
            while start_time + self.chunk_size <= uem_end:
                chunk_info = {
                    'session_id': session_id,
                    'audio_path': audio_path,
                    'start_time': start_time,
                    'end_time': start_time + self.chunk_size,
                    'chunk_id': f"{session_id}_{start_time:.1f}_{start_time+self.chunk_size:.1f}"
                }
                chunks.append(chunk_info)
                start_time += self.chunk_shift

        return chunks

    def __getitem__(self, idx):
        chunk_info = self.chunks[idx]

        # åŠ è½½éŸ³é¢‘å—
        waveform = self._load_audio_chunk(chunk_info)

        # æ•°æ®å¢å¼º
        if self.training:
            waveform = self.audio_augment(waveform)

        # åŠ è½½æ ‡æ³¨
        labels = self._load_labels_for_chunk(chunk_info)

        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        return {
            'waveform': waveform,      # [1, chunk_samples]
            'labels': labels,          # [chunk_frames, num_classes]
            'chunk_info': chunk_info   # å…ƒä¿¡æ¯
        }
```

#### è®­ç»ƒç›‘æ§å’Œå¯è§†åŒ–

**TensorBoardç›‘æ§é…ç½®**ï¼š

```python
class TrainingMonitor:
    def __init__(self, log_dir):
        self.writer = SummaryWriter(log_dir)

        # ç›‘æ§æŒ‡æ ‡
        self.metrics = {
            'train_loss': [],
            'val_loss': [],
            'val_der': [],
            'learning_rate_small': [],
            'learning_rate_big': [],
            'gradient_norm_small': [],
            'gradient_norm_big': []
        }

    def log_epoch(self, epoch, train_metrics, val_metrics, lr_small, lr_big):
        """è®°å½•æ¯ä¸ªepochçš„æŒ‡æ ‡"""

        # è®­ç»ƒæŒ‡æ ‡
        self.writer.add_scalar('train/loss', train_metrics['loss'], epoch)
        self.writer.add_scalar('train/ce_loss', train_metrics['ce_loss'], epoch)
        if 'distill_loss' in train_metrics:
            self.writer.add_scalar('train/distill_loss', train_metrics['distill_loss'], epoch)

        # éªŒè¯æŒ‡æ ‡
        self.writer.add_scalar('val/loss', val_metrics['loss'], epoch)
        self.writer.add_scalar('val/der', val_metrics['der'], epoch)
        self.writer.add_scalar('val/miss', val_metrics['miss'], epoch)
        self.writer.add_scalar('val/false_alarm', val_metrics['false_alarm'], epoch)
        self.writer.add_scalar('val/confusion', val_metrics['confusion'], epoch)

        # å­¦ä¹ ç‡
        self.writer.add_scalar('lr/wavlm', lr_small, epoch)
        self.writer.add_scalar('lr/conformer', lr_big, epoch)

        # æ¢¯åº¦èŒƒæ•°
        if 'grad_norm_small' in train_metrics:
            self.writer.add_scalar('grad_norm/wavlm', train_metrics['grad_norm_small'], epoch)
        if 'grad_norm_big' in train_metrics:
            self.writer.add_scalar('grad_norm/conformer', train_metrics['grad_norm_big'], epoch)

    def log_step(self, step, loss, lr_small, lr_big):
        """è®°å½•æ¯ä¸ªstepçš„æŒ‡æ ‡ï¼ˆå¯é€‰ï¼Œç”¨äºè¯¦ç»†ç›‘æ§ï¼‰"""
        self.writer.add_scalar('train/step_loss', loss, step)
        self.writer.add_scalar('lr/step_wavlm', lr_small, step)
        self.writer.add_scalar('lr/step_conformer', lr_big, step)
```

#### è®­ç»ƒå¯åŠ¨è„šæœ¬

**å•GPUè®­ç»ƒ**ï¼š
```bash
#!/bin/bash
# å•GPUè®­ç»ƒè„šæœ¬

export CUDA_VISIBLE_DEVICES=0

python recipes/diar_ssl/run_dual_opt.py \
    -C recipes/diar_ssl/conf/wavlm_updated_conformer.toml \
    -M train \
    --debug false \
    --resume ""  # ä»å¤´å¼€å§‹è®­ç»ƒ
```

**å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ**ï¼š
```bash
#!/bin/bash
# å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬

export CUDA_VISIBLE_DEVICES=0,1,2,3
export MASTER_ADDR=localhost
export MASTER_PORT=12345
export WORLD_SIZE=4
export RANK=0

accelerate launch \
    --num_processes 4 \
    --main_process_port 12345 \
    --multi_gpu \
    recipes/diar_ssl/run_dual_opt.py \
    -C recipes/diar_ssl/conf/wavlm_updated_conformer.toml \
    -M train
```

**ç»§ç»­è®­ç»ƒ**ï¼š
```bash
#!/bin/bash
# ä»æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ

python recipes/diar_ssl/run_dual_opt.py \
    -C recipes/diar_ssl/conf/wavlm_updated_conformer.toml \
    -M train \
    --resume exp/wavlm_conformer_exp/checkpoints/epoch_050.pt
```

### å¯åŠ¨è®­ç»ƒ

#### å•GPUè®­ç»ƒ
```bash
python run_dual_opt.py -C conf/wavlm_updated_conformer.toml -M train
```

#### å¤šGPUè®­ç»ƒ
```bash
CUDA_VISIBLE_DEVICES="0,1,2,3" accelerate launch \
    --num_processes 4 --main_process_port 1134 \
    run_dual_opt.py -C conf/wavlm_updated_conformer.toml -M train
```

#### è®­ç»ƒè„šæœ¬å‚æ•°
```bash
python run_dual_opt.py \
    -C config_file.toml \           # é…ç½®æ–‡ä»¶è·¯å¾„
    -M train \                      # æ¨¡å¼ï¼štrain/validate
    --resume checkpoint.pt \        # æ¢å¤è®­ç»ƒï¼ˆå¯é€‰ï¼‰
    --debug                         # è°ƒè¯•æ¨¡å¼ï¼ˆå¯é€‰ï¼‰
```

### ç›‘æ§è®­ç»ƒè¿‡ç¨‹

#### TensorBoardæ—¥å¿—
```bash
tensorboard --logdir exp/wavlm_updated_conformer/logs
```

**å¯è§†åŒ–æŒ‡æ ‡**ï¼š
- è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿
- å­¦ä¹ ç‡å˜åŒ–
- æ¢¯åº¦èŒƒæ•°
- DERåˆ†æ•°

#### æ£€æŸ¥ç‚¹ç®¡ç†
```python
# è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨ä¿å­˜çš„æ–‡ä»¶
exp/wavlm_updated_conformer/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ epoch_001.pt
â”‚   â”œâ”€â”€ epoch_002.pt
â”‚   â””â”€â”€ best_model.pt
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ tensorboard_logs/
â””â”€â”€ config.toml
```

### æ¨¡å‹è¯„ä¼°

#### éªŒè¯é›†è¯„ä¼°
```bash
python infer_avg.py \
    -C exp/wavlm_updated_conformer/config.toml \
    -i data/dev/wav.scp \
    -o results/dev \
    --embedding_model /path/to/embedding_model.bin \
    --avg_ckpt_num 5 \
    --val_metric Loss \
    --val_mode best
```

#### æµ‹è¯•é›†æ¨ç†
```bash
# å®Œæ•´çš„æ¨ç†+è¯„ä¼°æµç¨‹
bash recipes/diar_ssl/run_stage.sh
```

---

## æ¨¡å‹å‰ªæ

### å‰ªæåŸç†

ç»“æ„åŒ–å‰ªæé€šè¿‡ç§»é™¤æ•´ä¸ªç¥ç»å…ƒã€æ³¨æ„åŠ›å¤´æˆ–å±‚æ¥å‹ç¼©æ¨¡å‹ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹ç»“æ„çš„å®Œæ•´æ€§ã€‚

#### å‰ªæç­–ç•¥
1. **é‡è¦æ€§è¯„ä¼°**ï¼šè®¡ç®—æ¯ä¸ªç»“æ„å•å…ƒçš„é‡è¦æ€§åˆ†æ•°
2. **æ¸è¿›å‰ªæ**ï¼šé€æ­¥ç§»é™¤ä¸é‡è¦çš„ç»“æ„
3. **çŸ¥è¯†è’¸é¦**ï¼šä½¿ç”¨åŸæ¨¡å‹æŒ‡å¯¼å‰ªææ¨¡å‹è®­ç»ƒ
4. **å¾®è°ƒæ¢å¤**ï¼šå‰ªæåç»§ç»­è®­ç»ƒæ¢å¤æ€§èƒ½

### å‰ªæé…ç½®

#### è’¸é¦é…ç½®æ–‡ä»¶
```toml
[distill]
teacher_model_path = "exp/teacher_model/best_model.pt"
student_sparsity = 0.8                  # å‰ªææ¯”ä¾‹ï¼ˆ80%ï¼‰
distill_loss_weight = 1.0               # è’¸é¦æŸå¤±æƒé‡

[distill_loss]
l2_weight = 1.0                         # L2æŸå¤±æƒé‡
l1_weight = 0.1                         # L1æŸå¤±æƒé‡  
cos_weight = 0.1                        # ä½™å¼¦æŸå¤±æƒé‡
cos_type = "raw"                        # ä½™å¼¦æŸå¤±ç±»å‹

[pruning]
pruning_method = "magnitude"            # å‰ªææ–¹æ³•
structured = true                       # ç»“æ„åŒ–å‰ªæ
global_pruning = true                   # å…¨å±€å‰ªæ
```

### å‰ªææµç¨‹

#### 1. å‡†å¤‡æ•™å¸ˆæ¨¡å‹
```bash
# é¦–å…ˆè®­ç»ƒä¸€ä¸ªå®Œæ•´çš„æ•™å¸ˆæ¨¡å‹
bash recipes/diar_ssl/run_stage.sh
```

#### 2. æ‰§è¡Œå‰ªæè®­ç»ƒ
```bash
cd recipes/diar_ssl_pruning

# å¯åŠ¨å‰ªæè®­ç»ƒ
CUDA_VISIBLE_DEVICES="0,1" accelerate launch \
    --num_processes 2 --main_process_port 1135 \
    run_distill_prune.py -C conf/distill_prune_80.toml -M train
```

#### 3. åº”ç”¨å‰ªæ
```bash
python apply_pruning.py \
    --model_path exp/student_model/best_model.pt \
    --sparsity 0.8 \
    --output_path pruned_model.pt
```

### å‰ªææ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ç‰ˆæœ¬ | å‚æ•°é‡ | è®¡ç®—é‡(MACs) | æ¨ç†é€Ÿåº¦ | AMI DER | ç›¸å¯¹æ€§èƒ½ |
|----------|--------|--------------|----------|---------|----------|
| WavLM Base+ | 94.4M | 6.9G | 1.0Ã— | 15.6% | 100% |
| å‰ªæ 80% | 18.8M | 1.1G | **4.0Ã—** | 15.7% | **99.4%** |
| å‰ªæ 90% | 9.4M | 0.6G | **5.7Ã—** | 17.2% | **90.6%** |

#### å‰ªææ•ˆæœåˆ†æ
```python
def analyze_pruning_results(original_model, pruned_model):
    """åˆ†æå‰ªææ•ˆæœ"""
    
    # å‚æ•°é‡å¯¹æ¯”
    orig_params = sum(p.numel() for p in original_model.parameters())
    pruned_params = sum(p.numel() for p in pruned_model.parameters())
    compression_ratio = pruned_params / orig_params
    
    print(f"åŸå§‹å‚æ•°é‡: {orig_params:,}")
    print(f"å‰ªæåå‚æ•°é‡: {pruned_params:,}")
    print(f"å‹ç¼©æ¯”: {compression_ratio:.2%}")
    
    # è®¡ç®—ç¨€ç–åº¦
    total_weights = 0
    zero_weights = 0
    
    for param in pruned_model.parameters():
        if param.requires_grad:
            total_weights += param.numel()
            zero_weights += (param == 0).sum().item()
    
    sparsity = zero_weights / total_weights
    print(f"ç¨€ç–åº¦: {sparsity:.2%}")
```

### è‡ªå®šä¹‰å‰ªæç­–ç•¥

#### åŸºäºé‡è¦æ€§çš„å‰ªæ
```python
class ImportanceBasedPruning:
    def __init__(self, model, sparsity=0.8):
        self.model = model
        self.sparsity = sparsity
        
    def compute_importance(self, layer):
        """è®¡ç®—å±‚é‡è¦æ€§åˆ†æ•°"""
        if hasattr(layer, 'weight'):
            # L2èŒƒæ•°é‡è¦æ€§
            l2_importance = torch.norm(layer.weight, dim=1)
            
            # æ¢¯åº¦é‡è¦æ€§ï¼ˆéœ€è¦åœ¨è®­ç»ƒä¸­ç§¯ç´¯ï¼‰
            if hasattr(layer.weight, 'grad') and layer.weight.grad is not None:
                grad_importance = torch.norm(layer.weight.grad, dim=1)
                importance = l2_importance * grad_importance
            else:
                importance = l2_importance
                
            return importance
        return None
        
    def prune_layer(self, layer, importance_scores):
        """å‰ªæå•ä¸ªå±‚"""
        num_keep = int(len(importance_scores) * (1 - self.sparsity))
        _, indices = torch.topk(importance_scores, num_keep)
        
        # åˆ›å»ºæ©ç 
        mask = torch.zeros_like(importance_scores, dtype=torch.bool)
        mask[indices] = True
        
        return mask
```

---

## è¯„ä¼°ä¸åŸºå‡†æµ‹è¯•

### è¯„ä¼°æŒ‡æ ‡

#### DER (Diarization Error Rate)
DERæ˜¯è¯´è¯äººåˆ†ç¦»çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ï¼š

```
DER = (è¯´è¯äººé”™è¯¯æ—¶é—´ + é—æ¼æ—¶é—´ + è™šå‡æ£€æµ‹æ—¶é—´) / æ€»è¯´è¯æ—¶é—´ Ã— 100%
```

**ç»„æˆéƒ¨åˆ†**ï¼š
- **è¯´è¯äººé”™è¯¯**ï¼šå°†è¯´è¯äººAè¯¯è¯†åˆ«ä¸ºè¯´è¯äººBçš„æ—¶é—´
- **é—æ¼**ï¼šæœ‰äººè¯´è¯ä½†ç³»ç»Ÿæœªæ£€æµ‹åˆ°çš„æ—¶é—´  
- **è™šå‡æ£€æµ‹**ï¼šæ— äººè¯´è¯ä½†ç³»ç»Ÿæ£€æµ‹åˆ°è¯´è¯çš„æ—¶é—´

#### è®¡ç®—ç¤ºä¾‹
```python
def calculate_der(reference_rttm, hypothesis_rttm, collar=0.0):
    """
    è®¡ç®—DERåˆ†æ•°
    
    Args:
        reference_rttm: æ ‡å‡†ç­”æ¡ˆRTTMæ–‡ä»¶è·¯å¾„
        hypothesis_rttm: ç³»ç»Ÿè¾“å‡ºRTTMæ–‡ä»¶è·¯å¾„  
        collar: å®¹å¿åŒºé—´ï¼ˆç§’ï¼‰ï¼Œé€šå¸¸ä¸º0æˆ–0.25
    """
    from pyannote.metrics.diarization import DiarizationErrorRate
    
    # åŠ è½½æ ‡æ³¨
    reference = load_rttm(reference_rttm)
    hypothesis = load_rttm(hypothesis_rttm)
    
    # è®¡ç®—DER
    metric = DiarizationErrorRate(collar=collar)
    
    for uri in reference.uris:
        ref_annotation = reference[uri]
        hyp_annotation = hypothesis[uri]
        metric(ref_annotation, hyp_annotation, uem=uri)
    
    # è·å–è¯¦ç»†ç»“æœ
    der_components = metric.report(display=False)
    total_der = abs(metric)
    
    return {
        'total_der': total_der,
        'confusion': der_components['confusion / total'],
        'miss': der_components['miss / total'], 
        'false_alarm': der_components['false alarm / total']
    }
```

### åŸºå‡†æ•°æ®é›†

#### 1. AMI Meeting Corpus
- **æè¿°**ï¼šè‹±æ–‡ä¼šè®®å½•éŸ³ï¼Œ4äººå‚ä¸
- **ç‰¹ç‚¹**ï¼šå¤šæ¨¡æ€ï¼ˆéŸ³é¢‘+è§†é¢‘ï¼‰ï¼Œè¿œåœºå½•éŸ³
- **éš¾ç‚¹**ï¼šé‡å è¯´è¯ï¼Œå™ªå£°ç¯å¢ƒ

#### 2. AISHELL-4
- **æè¿°**ï¼šä¸­æ–‡ä¼šè®®å½•éŸ³
- **ç‰¹ç‚¹**ï¼š8å£°é“å½•éŸ³ï¼Œè½¬å•å£°é“è¯„ä¼°
- **éš¾ç‚¹**ï¼šä¸­æ–‡è¯­éŸ³ç‰¹æ€§ï¼Œå£éŸ³å˜åŒ–

#### 3. AliMeeting
- **æè¿°**ï¼šé˜¿é‡Œå·´å·´ä¸­æ–‡ä¼šè®®æ•°æ®é›†
- **ç‰¹ç‚¹**ï¼šè¿œåœºå½•éŸ³ï¼Œå¤šæ ·åŒ–åœºæ™¯
- **éš¾ç‚¹**ï¼šçœŸå®ä¼šè®®ç¯å¢ƒï¼Œå¤æ‚å£°å­¦æ¡ä»¶

#### 4. VoxConverse
- **æè¿°**ï¼šä»VoxCelebæå–çš„å¯¹è¯æ•°æ®
- **ç‰¹ç‚¹**ï¼šç”µè¯è´¨é‡éŸ³é¢‘ï¼Œ2-3äººå¯¹è¯
- **éš¾ç‚¹**ï¼šéŸ³è´¨è¾ƒå·®ï¼Œä¿¡é“å¤±çœŸ

### è¯„ä¼°è„šæœ¬

#### å®Œæ•´è¯„ä¼°æµç¨‹
```bash
#!/bin/bash
# å®Œæ•´çš„è¯„ä¼°è„šæœ¬

# è®¾ç½®è·¯å¾„
DIARIZATION_DIR="exp/wavlm_updated_conformer"
DATA_DIR="data"
OUTPUT_DIR="evaluation_results"

# æ•°æ®é›†åˆ—è¡¨
datasets=("AMI" "AISHELL4" "AliMeeting" "VoxConverse")

for dataset in "${datasets[@]}"; do
    echo "è¯„ä¼°æ•°æ®é›†: $dataset"
    
    # æ‰§è¡Œæ¨ç†
    python infer_avg.py \
        -C $DIARIZATION_DIR/config.toml \
        -i $DATA_DIR/test/$dataset/wav.scp \
        -o $OUTPUT_DIR/$dataset \
        --embedding_model /path/to/embedding_model.bin \
        --avg_ckpt_num 5
    
    # è®¡ç®—DER
    python dscore/score.py \
        -r $DATA_DIR/test/$dataset/rttm \
        -s $OUTPUT_DIR/$dataset/*.rttm \
        --collar 0 \
        > $OUTPUT_DIR/$dataset/der_results.txt
    
    # æå–DERåˆ†æ•°
    der_score=$(grep "OVERALL" $OUTPUT_DIR/$dataset/der_results.txt | awk '{print $4}')
    echo "$dataset DER: $der_score%"
done
```

#### æ€§èƒ½åˆ†æè„šæœ¬
```python
import pandas as pd
import matplotlib.pyplot as plt
from pathlib import Path

def analyze_performance(results_dir):
    """åˆ†æè¯„ä¼°ç»“æœ"""
    
    results = {}
    
    # è¯»å–å„ä¸ªæ•°æ®é›†çš„ç»“æœ
    for dataset_dir in Path(results_dir).iterdir():
        if dataset_dir.is_dir():
            dataset = dataset_dir.name
            der_file = dataset_dir / "der_results.txt"
            
            if der_file.exists():
                with open(der_file) as f:
                    lines = f.readlines()
                
                # è§£æDERç»“æœ
                for line in lines:
                    if "OVERALL" in line:
                        parts = line.split()
                        total_der = float(parts[3])
                        miss = float(parts[4]) 
                        falarm = float(parts[5])
                        confusion = float(parts[6])
                        
                        results[dataset] = {
                            'total_der': total_der,
                            'miss': miss,
                            'false_alarm': falarm, 
                            'confusion': confusion
                        }
    
    # åˆ›å»ºç»“æœè¡¨æ ¼
    df = pd.DataFrame(results).T
    print("æ€§èƒ½è¯„ä¼°ç»“æœ:")
    print(df.round(2))
    
    # ç»˜åˆ¶å¯¹æ¯”å›¾
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # DERæ€»åˆ†
    df['total_der'].plot(kind='bar', ax=axes[0,0], title='Total DER')
    
    # å„é¡¹é”™è¯¯åˆ†è§£
    df[['miss', 'false_alarm', 'confusion']].plot(kind='bar', ax=axes[0,1], title='Error Breakdown')
    
    # ä¸åŸºçº¿å¯¹æ¯”ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    baseline_results = {
        'AMI': 22.4, 'AISHELL4': 12.2, 
        'AliMeeting': 24.4, 'VoxConverse': 11.3
    }
    
    comparison_data = pd.DataFrame({
        'DiariZen': df['total_der'],
        'Pyannote3.1': [baseline_results.get(k, 0) for k in df.index]
    })
    
    comparison_data.plot(kind='bar', ax=axes[1,0], title='vs Baseline')
    
    # ç›¸å¯¹æ”¹è¿›
    improvement = (comparison_data['Pyannote3.1'] - comparison_data['DiariZen']) / comparison_data['Pyannote3.1'] * 100
    improvement.plot(kind='bar', ax=axes[1,1], title='Relative Improvement (%)')
    
    plt.tight_layout()
    plt.savefig(f"{results_dir}/performance_analysis.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    return df

# ä½¿ç”¨ç¤ºä¾‹
results_df = analyze_performance("evaluation_results")
```

### ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•

```python
from scipy import stats
import numpy as np

def significance_test(results_a, results_b, alpha=0.05):
    """
    æ£€éªŒä¸¤ä¸ªç³»ç»Ÿæ€§èƒ½å·®å¼‚çš„æ˜¾è‘—æ€§
    
    Args:
        results_a: ç³»ç»ŸAåœ¨å„ä¸ªä¼šè¯ä¸Šçš„DERåˆ†æ•°åˆ—è¡¨
        results_b: ç³»ç»ŸBåœ¨å„ä¸ªä¼šè¯ä¸Šçš„DERåˆ†æ•°åˆ—è¡¨
        alpha: æ˜¾è‘—æ€§æ°´å¹³
    """
    
    # é…å¯¹tæ£€éªŒ
    t_stat, p_value = stats.ttest_rel(results_a, results_b)
    
    # æ•ˆæœé‡ï¼ˆCohen's dï¼‰
    diff = np.array(results_a) - np.array(results_b)
    cohen_d = np.mean(diff) / np.std(diff, ddof=1)
    
    # Wilcoxonç¬¦å·ç§©æ£€éªŒï¼ˆéå‚æ•°ï¼‰
    w_stat, w_p_value = stats.wilcoxon(results_a, results_b)
    
    print(f"é…å¯¹tæ£€éªŒ: t={t_stat:.3f}, p={p_value:.3f}")
    print(f"Cohen's d: {cohen_d:.3f}")
    print(f"Wilcoxonæ£€éªŒ: W={w_stat:.3f}, p={w_p_value:.3f}")
    
    if p_value < alpha:
        print(f"å·®å¼‚æ˜¾è‘— (p < {alpha})")
    else:
        print(f"å·®å¼‚ä¸æ˜¾è‘— (p >= {alpha})")
    
    return {
        't_statistic': t_stat,
        'p_value': p_value,
        'cohen_d': cohen_d,
        'wilcoxon_statistic': w_stat,
        'wilcoxon_p_value': w_p_value
    }
```

---

## é«˜çº§é…ç½®

### è‡ªå®šä¹‰æ¨¡å‹æ¶æ„

#### ä¿®æ”¹Conformeré…ç½®
```python
# åœ¨é…ç½®æ–‡ä»¶ä¸­è°ƒæ•´æ¨¡å‹å‚æ•°
[model.args]
attention_in = 512           # å¢åŠ æ³¨æ„åŠ›ç»´åº¦
ffn_hidden = 2048           # å¢åŠ å‰é¦ˆç½‘ç»œå¤§å°
num_head = 8                # å¢åŠ æ³¨æ„åŠ›å¤´æ•°
num_layer = 6               # å¢åŠ ç¼–ç å™¨å±‚æ•°
kernel_size = 31            # å·ç§¯æ ¸å¤§å°
dropout = 0.15              # å¢åŠ Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ
use_posi = true             # å¯ç”¨ä½ç½®ç¼–ç 
```

#### WavLMå±‚çº§é€‰æ‹©
```python
# è‡ªå®šä¹‰WavLMå±‚çº§åŠ æƒ
class CustomWavLMWeighting(nn.Module):
    def __init__(self, num_layers=13):
        super().__init__()
        # å¯å­¦ä¹ çš„å±‚çº§æƒé‡
        self.layer_weights = nn.Parameter(torch.ones(num_layers))
        # æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶æƒé‡åˆ†å¸ƒçš„é”åº¦
        self.temperature = nn.Parameter(torch.tensor(1.0))
        
    def forward(self, layer_outputs):
        # è®¡ç®—softmaxæƒé‡
        weights = F.softmax(self.layer_weights / self.temperature, dim=0)
        
        # åŠ æƒèåˆ
        weighted_output = sum(w * layer for w, layer in zip(weights, layer_outputs))
        return weighted_output
```

### æ•°æ®å¢å¼ºç­–ç•¥

#### éŸ³é¢‘å¢å¼º
```python
import torch.nn.functional as F
import torchaudio.transforms as T

class AudioAugmentation:
    def __init__(self, sample_rate=16000):
        self.sample_rate = sample_rate
        
        # é€Ÿåº¦æ‰°åŠ¨
        self.speed_perturb = T.SpeedPerturbation(
            sample_rate, factors=[0.9, 1.0, 1.1]
        )
        
        # éŸ³é‡æ‰°åŠ¨
        self.vol_perturb = T.Vol(gain_type="amplitude")
        
        # æ·»åŠ å™ªå£°
        self.add_noise = T.AddNoise()
        
    def __call__(self, waveform, augment_prob=0.5):
        if torch.rand(1) < augment_prob:
            # éšæœºé€‰æ‹©å¢å¼ºæ–¹æ³•
            aug_type = torch.randint(0, 3, (1,)).item()
            
            if aug_type == 0:
                # é€Ÿåº¦æ‰°åŠ¨
                waveform = self.speed_perturb(waveform)
            elif aug_type == 1:
                # éŸ³é‡æ‰°åŠ¨
                gain = torch.uniform(-3, 3, (1,))  # dB
                waveform = self.vol_perturb(waveform, gain)
            else:
                # æ·»åŠ ç™½å™ªå£°
                noise_level = torch.uniform(0.001, 0.01, (1,))
                noise = torch.randn_like(waveform) * noise_level
                waveform = waveform + noise
                
        return waveform
```

#### æ ‡æ³¨å¢å¼º
```python
class LabelAugmentation:
    def __init__(self, label_smooth=0.1, mixup_alpha=0.2):
        self.label_smooth = label_smooth
        self.mixup_alpha = mixup_alpha
        
    def label_smoothing(self, labels, num_classes):
        """æ ‡ç­¾å¹³æ»‘"""
        smooth_labels = labels * (1 - self.label_smooth)
        smooth_labels += self.label_smooth / num_classes
        return smooth_labels
        
    def mixup(self, waveform1, labels1, waveform2, labels2):
        """Mixupæ•°æ®å¢å¼º"""
        lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)
        
        # æ··åˆéŸ³é¢‘
        mixed_waveform = lam * waveform1 + (1 - lam) * waveform2
        
        # æ··åˆæ ‡ç­¾  
        mixed_labels = lam * labels1 + (1 - lam) * labels2
        
        return mixed_waveform, mixed_labels
```

### æŸå¤±å‡½æ•°å®šåˆ¶

#### ç„¦ç‚¹æŸå¤±ï¼ˆFocal Lossï¼‰
```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
    def forward(self, inputs, targets):
        ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss
```

#### æ—¶åºä¸€è‡´æ€§æŸå¤±
```python
class TemporalConsistencyLoss(nn.Module):
    def __init__(self, weight=0.1):
        super().__init__()
        self.weight = weight
        
    def forward(self, predictions):
        # è®¡ç®—ç›¸é‚»å¸§é¢„æµ‹çš„å·®å¼‚
        diff = predictions[:, 1:] - predictions[:, :-1]
        
        # L2æ­£åˆ™åŒ–ï¼Œé¼“åŠ±å¹³æ»‘å˜åŒ–
        consistency_loss = torch.mean(diff ** 2)
        
        return self.weight * consistency_loss
```

### ä¼˜åŒ–ç­–ç•¥

#### å­¦ä¹ ç‡è°ƒåº¦
```python
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts

# ä½™å¼¦é€€ç«withçƒ­é‡å¯
scheduler = CosineAnnealingWarmRestarts(
    optimizer, 
    T_0=10,      # ç¬¬ä¸€æ¬¡é‡å¯çš„å‘¨æœŸ
    T_mult=2,    # é‡å¯å‘¨æœŸçš„å€æ•°
    eta_min=1e-6 # æœ€å°å­¦ä¹ ç‡
)

# å¸¦é¢„çƒ­çš„çº¿æ€§è¡°å‡
from transformers import get_linear_schedule_with_warmup

scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=1000,     # é¢„çƒ­æ­¥æ•°
    num_training_steps=50000   # æ€»è®­ç»ƒæ­¥æ•°
)
```

#### æ¢¯åº¦è£å‰ª
```python
# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ æ¢¯åº¦è£å‰ª
def training_step(model, batch, optimizer):
    optimizer.zero_grad()
    
    outputs = model(batch)
    loss = compute_loss(outputs, batch['targets'])
    
    loss.backward()
    
    # æ¢¯åº¦è£å‰ª
    torch.nn.utils.clip_grad_norm_(
        model.parameters(), 
        max_norm=1.0  # æœ€å¤§æ¢¯åº¦èŒƒæ•°
    )
    
    optimizer.step()
    
    return loss.item()
```

---

## å¸¸è§é—®é¢˜

### å®‰è£…é—®é¢˜

#### Q1: CUDAç‰ˆæœ¬ä¸åŒ¹é…
```bash
# é”™è¯¯ä¿¡æ¯ï¼šRuntimeError: CUDA version mismatch
# è§£å†³æ–¹æ¡ˆï¼šæ£€æŸ¥CUDAç‰ˆæœ¬å¹¶å®‰è£…å¯¹åº”çš„PyTorch
nvidia-smi  # æŸ¥çœ‹CUDAç‰ˆæœ¬

# å¯¹äºCUDA 11.8
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# å¯¹äºCUDA 12.1  
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
```

#### Q2: å†…å­˜ä¸è¶³
```python
# é”™è¯¯ä¿¡æ¯ï¼šRuntimeError: CUDA out of memory
# è§£å†³æ–¹æ¡ˆï¼šå‡å°‘æ‰¹å¤„ç†å¤§å°

# åœ¨é…ç½®æ–‡ä»¶ä¸­è°ƒæ•´
[train_dataset.dataloader]
batch_size = 8  # ä»16é™åˆ°8

[validate_dataset.dataloader] 
batch_size = 4  # ä»8é™åˆ°4

# æˆ–å¯ç”¨æ¢¯åº¦ç´¯ç§¯
[trainer.args]
gradient_accumulation_steps = 2  # ç´¯ç§¯2æ­¥å†æ›´æ–°
```

#### Q3: ä¾èµ–åŒ…å†²çª
```bash
# åˆ›å»ºå…¨æ–°ç¯å¢ƒé¿å…å†²çª
conda create --name diarizen_clean python=3.10
conda activate diarizen_clean

# ä¸¥æ ¼æŒ‰ç…§requirements.txtå®‰è£…
pip install -r requirements.txt --no-deps
pip install -e . --no-deps

# å†å•ç‹¬å®‰è£…ç¼ºå¤±çš„æ ¸å¿ƒä¾èµ–
pip install torch torchaudio accelerate
```

### è®­ç»ƒé—®é¢˜

#### Q4: æŸå¤±ä¸æ”¶æ•›
```python
# å¯èƒ½åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š

# 1. å­¦ä¹ ç‡è¿‡å¤§
[optimizer_big.args]
lr = 5e-4  # ä»1e-3é™åˆ°5e-4

# 2. æ•°æ®æ ‡æ³¨é—®é¢˜
# æ£€æŸ¥RTTMæ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®

# 3. æ·»åŠ å­¦ä¹ ç‡é¢„çƒ­
[trainer.args]
warmup_steps = 1000

# 4. æ£€æŸ¥æ•°æ®åŠ è½½
def debug_dataset(dataset):
    sample = dataset[0]
    print(f"Audio shape: {sample[0].shape}")
    print(f"Label shape: {sample[1].shape}")
    print(f"Label sum: {sample[1].sum()}")  # åº”è¯¥>0
```

#### Q5: éªŒè¯æ€§èƒ½å·®
```python
# æ£€æŸ¥éªŒè¯æ•°æ®çš„chunk_shiftè®¾ç½®
[validate_dataset.args]
chunk_shift = 8  # éªŒè¯æ—¶ä¸è¦é‡å ï¼Œä½¿ç”¨chunk_sizeå¤§å°çš„shift

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆ
[trainer.args]
max_patience = 5      # å‡å°‘patience
dropout = 0.2         # å¢åŠ dropout

[model.args]
dropout = 0.2
```

### æ¨ç†é—®é¢˜

#### Q6: æ¨ç†é€Ÿåº¦æ…¢
```python
# ä¼˜åŒ–å»ºè®®ï¼š

# 1. è°ƒæ•´æ‰¹å¤„ç†å¤§å°
pipeline_config = {
    "inference": {
        "args": {
            "batch_size": 64,  # å¢åŠ batch size
            "seg_duration": 8   # å‡å°‘æ®µé•¿åº¦
        }
    }
}

# 2. ä½¿ç”¨å‰ªææ¨¡å‹
pipeline = DiariZenPipeline.from_pretrained(
    "BUT-FIT/diarizen-wavlm-base-s80-md"  # ä½¿ç”¨baseè€Œélarge
)

# 3. å…³é—­ä¸å¿…è¦çš„å¤„ç†
pipeline_config = {
    "inference": {
        "args": {
            "apply_median_filtering": False  # å…³é—­ä¸­å€¼æ»¤æ³¢
        }
    }
}
```

#### Q7: è¯´è¯äººæ•°é‡ä¸å‡†ç¡®
```python
# è°ƒæ•´èšç±»å‚æ•°ï¼š

# å¯¹äºVBxèšç±»
vbx_config = {
    "Fa": 0.05,    # å‡å°‘Faå¢åŠ è¯´è¯äººæ•°
    "Fb": 0.9,     # å¢åŠ Fbå‡å°‘è¯´è¯äººæ•°
    "ahc_threshold": 0.5  # é™ä½é˜ˆå€¼å¢åŠ è¯´è¯äººæ•°
}

# å¯¹äºå±‚æ¬¡èšç±»
ahc_config = {
    "ahc_threshold": 0.6,      # é™ä½é˜ˆå€¼å¢åŠ è¯´è¯äººæ•°
    "min_cluster_size": 20     # å‡å°‘æœ€å°èšç±»å¤§å°
}

# è°ƒæ•´è¯´è¯äººæ•°é‡èŒƒå›´
pipeline_config = {
    "clustering": {
        "args": {
            "min_speakers": 2,    # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
            "max_speakers": 6     # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        }
    }
}
```

### æ•°æ®é—®é¢˜

#### Q8: RTTMæ ¼å¼é”™è¯¯
```python
# æ­£ç¡®çš„RTTMæ ¼å¼æ£€æŸ¥
def validate_rttm(rttm_file):
    with open(rttm_file, 'r') as f:
        for line_num, line in enumerate(f, 1):
            parts = line.strip().split()
            
            # RTTMåº”è¯¥æœ‰10ä¸ªå­—æ®µ
            if len(parts) != 10:
                print(f"Line {line_num}: Wrong number of fields ({len(parts)})")
                
            # ç¬¬ä¸€ä¸ªå­—æ®µåº”è¯¥æ˜¯SPEAKER
            if parts[0] != 'SPEAKER':
                print(f"Line {line_num}: First field should be 'SPEAKER'")
                
            # æ£€æŸ¥æ—¶é—´æ ¼å¼
            try:
                start_time = float(parts[3])
                duration = float(parts[4])
                if start_time < 0 or duration <= 0:
                    print(f"Line {line_num}: Invalid time values")
            except ValueError:
                print(f"Line {line_num}: Time values not numeric")

# ä½¿ç”¨ç¤ºä¾‹
validate_rttm("data/train/rttm")
```

#### Q9: éŸ³é¢‘æ ¼å¼é—®é¢˜
```python
import soundfile as sf
import torchaudio

def check_audio_format(audio_file):
    """æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ ¼å¼"""
    try:
        # ä½¿ç”¨soundfileè¯»å–
        data, sr = sf.read(audio_file)
        print(f"Soundfile - Shape: {data.shape}, Sample rate: {sr}")
        
        # ä½¿ç”¨torchaudioè¯»å–
        waveform, sample_rate = torchaudio.load(audio_file)
        print(f"Torchaudio - Shape: {waveform.shape}, Sample rate: {sample_rate}")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡é‡‡æ ·
        if sample_rate != 16000:
            print(f"Warning: Sample rate is {sample_rate}, expected 16000")
            
        # æ£€æŸ¥å£°é“æ•°
        if len(waveform.shape) > 1 and waveform.shape[0] > 1:
            print(f"Warning: Multi-channel audio ({waveform.shape[0]} channels)")
            
    except Exception as e:
        print(f"Error reading {audio_file}: {e}")

# æ‰¹é‡æ£€æŸ¥
import glob
for audio_file in glob.glob("data/train/*.wav"):
    check_audio_format(audio_file)
```

### æ€§èƒ½è°ƒä¼˜

#### Q10: å¦‚ä½•æé«˜DERæ€§èƒ½ï¼Ÿ
```python
# 1. æ•°æ®è´¨é‡ä¼˜åŒ–
# - ç¡®ä¿æ ‡æ³¨å‡†ç¡®æ€§
# - å¢åŠ è®­ç»ƒæ•°æ®é‡
# - æ•°æ®æ¸…æ´—ï¼Œç§»é™¤å™ªå£°æ ·æœ¬

# 2. æ¨¡å‹ä¼˜åŒ–
[model.args]
attention_in = 512       # å¢åŠ æ¨¡å‹å®¹é‡
num_layer = 6           # å¢åŠ å±‚æ•°
num_head = 8            # å¢åŠ æ³¨æ„åŠ›å¤´

# 3. è®­ç»ƒç­–ç•¥ä¼˜åŒ–
[trainer.args]
max_epochs = 150        # å¢åŠ è®­ç»ƒè½®æ•°
lr_decay = true         # å¯ç”¨å­¦ä¹ ç‡è¡°å‡

# 4. åå¤„ç†ä¼˜åŒ–
inference_config = {
    "apply_median_filtering": True,    # å¯ç”¨ä¸­å€¼æ»¤æ³¢
    "seg_duration": 16                 # å¢åŠ åˆ†æ®µé•¿åº¦
}

# 5. èšç±»å‚æ•°è°ƒä¼˜
# ä½¿ç”¨éªŒè¯é›†è¿›è¡Œç½‘æ ¼æœç´¢
def grid_search_clustering():
    thresholds = [0.5, 0.6, 0.7, 0.8]
    fa_values = [0.05, 0.07, 0.1]
    
    best_der = float('inf')
    best_params = {}
    
    for threshold in thresholds:
        for fa in fa_values:
            config = {
                "ahc_threshold": threshold,
                "Fa": fa
            }
            
            # è¿è¡Œæ¨ç†
            der = run_inference_with_config(config)
            
            if der < best_der:
                best_der = der
                best_params = config
                
    return best_params, best_der
```

---

## å¼€å‘è€…æŒ‡å—

### ä»£ç ç»“æ„

#### æ ¸å¿ƒæ¨¡å—ç»„ç»‡
```
diarizen/
â”œâ”€â”€ models/                  # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ eend/               # ç«¯åˆ°ç«¯æ¨¡å‹
â”‚   â”œâ”€â”€ module/             # åŸºç¡€æ¨¡å—  
â”‚   â””â”€â”€ pruning/            # å‰ªæç›¸å…³
â”œâ”€â”€ pipelines/              # æ¨ç†ç®¡é“
â”œâ”€â”€ clustering/             # èšç±»ç®—æ³•
â”œâ”€â”€ trainer_*.py           # è®­ç»ƒå™¨
â”œâ”€â”€ utils.py               # å·¥å…·å‡½æ•°
â””â”€â”€ optimization.py        # ä¼˜åŒ–ç›¸å…³
```

#### æ‰©å±•æ–°æ¨¡å‹
```python
# åˆ›å»ºæ–°çš„æ¨¡å‹ç±»
class MyCustomModel(BaseModel):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # è‡ªå®šä¹‰æ¶æ„
        self.feature_extractor = MyFeatureExtractor()
        self.encoder = MyEncoder()
        self.classifier = nn.Linear(hidden_dim, self.dimension)
        
    def forward(self, waveform):
        # å‰å‘ä¼ æ’­é€»è¾‘
        features = self.feature_extractor(waveform)
        encoded = self.encoder(features)
        logits = self.classifier(encoded)
        return logits
        
    @property
    def dimension(self):
        # è¿”å›è¾“å‡ºç»´åº¦
        return self.specifications.num_powerset_classes
        
    def get_rf_info(self):
        # è¿”å›æ„Ÿå—é‡ä¿¡æ¯
        return num_frames, duration, step
```

#### æ·»åŠ æ–°çš„èšç±»ç®—æ³•
```python
# åœ¨clustering/ç›®å½•ä¸‹æ·»åŠ æ–°ç®—æ³•
class MyClusteringAlgorithm:
    def __init__(self, **kwargs):
        self.kwargs = kwargs
        
    def __call__(self, embeddings, segmentations, min_clusters=1, max_clusters=20):
        """
        Args:
            embeddings: è¯´è¯äººåµŒå…¥å‘é‡
            segmentations: è¯­éŸ³æ´»åŠ¨æ£€æµ‹ç»“æœ
            min_clusters: æœ€å°èšç±»æ•°
            max_clusters: æœ€å¤§èšç±»æ•°
            
        Returns:
            hard_clusters: ç¡¬èšç±»ç»“æœ
            soft_clusters: è½¯èšç±»ç»“æœï¼ˆå¯é€‰ï¼‰
            details: å…¶ä»–ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        """
        
        # å®ç°èšç±»é€»è¾‘
        hard_clusters = self.cluster(embeddings)
        
        return hard_clusters, None, {}
        
    def cluster(self, embeddings):
        # å…·ä½“èšç±»å®ç°
        pass
```

### è‡ªå®šä¹‰æ•°æ®é›†

#### å®ç°Datasetç±»
```python
from torch.utils.data import Dataset

class MyDiarizationDataset(Dataset):
    def __init__(self, audio_dir, annotation_dir, **kwargs):
        self.audio_files = self.load_audio_list(audio_dir)
        self.annotations = self.load_annotations(annotation_dir)
        
    def __len__(self):
        return len(self.audio_files)
        
    def __getitem__(self, idx):
        # åŠ è½½éŸ³é¢‘
        audio_path = self.audio_files[idx]
        waveform, sample_rate = torchaudio.load(audio_path)
        
        # åŠ è½½æ ‡æ³¨
        annotation = self.annotations[idx]
        
        # é¢„å¤„ç†
        waveform = self.preprocess_audio(waveform, sample_rate)
        labels = self.preprocess_labels(annotation)
        
        return waveform, labels, audio_path
        
    def preprocess_audio(self, waveform, sample_rate):
        # é‡é‡‡æ ·åˆ°16kHz
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(sample_rate, 16000)
            waveform = resampler(waveform)
            
        # è½¬å•å£°é“
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
            
        return waveform
        
    def preprocess_labels(self, annotation):
        # å°†æ—¶é—´æ ‡æ³¨è½¬æ¢ä¸ºå¸§çº§æ ‡ç­¾
        # å®ç°å…·ä½“çš„æ ‡æ³¨å¤„ç†é€»è¾‘
        pass
```

### è°ƒè¯•å·¥å…·

#### æ¨¡å‹å¯è§†åŒ–
```python
from torchinfo import summary
import torch

def visualize_model(model, input_shape=(1, 1, 128000)):
    """å¯è§†åŒ–æ¨¡å‹ç»“æ„"""
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    dummy_input = torch.randn(input_shape)
    
    # æ‰“å°æ¨¡å‹æ‘˜è¦
    summary(model, input_size=input_shape, verbose=1)
    
    # åˆ†æè®¡ç®—å›¾
    with torch.no_grad():
        output = model(dummy_input)
        print(f"Output shape: {output.shape}")

# ä½¿ç”¨ç¤ºä¾‹
model = Model()
visualize_model(model)
```

#### è®­ç»ƒç›‘æ§
```python
import matplotlib.pyplot as plt
from tensorboard.backend.event_processing.event_accumulator import EventAccumulator

def plot_training_curves(log_dir):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    
    # è¯»å–TensorBoardæ—¥å¿—
    ea = EventAccumulator(log_dir)  
    ea.Reload()
    
    # è·å–æ ‡é‡æ•°æ®
    scalars = ea.Tags()['scalars']
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # è®­ç»ƒæŸå¤±
    if 'train/loss' in scalars:
        train_loss = ea.Scalars('train/loss')
        steps = [s.step for s in train_loss]
        values = [s.value for s in train_loss]
        axes[0,0].plot(steps, values, label='Train Loss')
        axes[0,0].set_title('Training Loss')
        axes[0,0].legend()
    
    # éªŒè¯æŸå¤±
    if 'val/loss' in scalars:
        val_loss = ea.Scalars('val/loss')
        steps = [s.step for s in val_loss]
        values = [s.value for s in val_loss]
        axes[0,1].plot(steps, values, label='Validation Loss', color='orange')
        axes[0,1].set_title('Validation Loss')
        axes[0,1].legend()
    
    # å­¦ä¹ ç‡
    if 'train/lr' in scalars:
        lr_data = ea.Scalars('train/lr')
        steps = [s.step for s in lr_data]
        values = [s.value for s in lr_data]
        axes[1,0].plot(steps, values, label='Learning Rate', color='green')
        axes[1,0].set_title('Learning Rate')
        axes[1,0].set_yscale('log')
        axes[1,0].legend()
    
    # DERåˆ†æ•°
    if 'val/der' in scalars:
        der_data = ea.Scalars('val/der')
        steps = [s.step for s in der_data]
        values = [s.value for s in der_data]
        axes[1,1].plot(steps, values, label='Validation DER', color='red')
        axes[1,1].set_title('Validation DER')
        axes[1,1].legend()
    
    plt.tight_layout()
    plt.savefig(f"{log_dir}/training_curves.png", dpi=300, bbox_inches='tight')
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
plot_training_curves("exp/wavlm_updated_conformer/logs")
```

#### é”™è¯¯è¯Šæ–­
```python
def diagnose_training_issues(config_file, checkpoint_file=None):
    """è¯Šæ–­è®­ç»ƒé—®é¢˜"""
    
    print("ğŸ” DiariZenè®­ç»ƒè¯Šæ–­")
    print("=" * 50)
    
    # 1. æ£€æŸ¥é…ç½®æ–‡ä»¶
    print("ğŸ“‹ é…ç½®æ£€æŸ¥:")
    config = toml.load(config_file)
    
    # å­¦ä¹ ç‡æ£€æŸ¥
    lr_big = config['optimizer_big']['args']['lr']
    lr_small = config['optimizer_small']['args']['lr']
    
    if lr_big < lr_small:
        print("âš ï¸  è­¦å‘Š: å¤§å­¦ä¹ ç‡ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡å°äºå°å­¦ä¹ ç‡ä¼˜åŒ–å™¨")
        
    if lr_big > 1e-2:
        print("âš ï¸  è­¦å‘Š: å¤§å­¦ä¹ ç‡å¯èƒ½è¿‡å¤§ï¼Œå®¹æ˜“å‘æ•£")
        
    # æ‰¹å¤„ç†å¤§å°æ£€æŸ¥
    batch_size = config['train_dataset']['dataloader']['batch_size']
    if batch_size < 4:
        print("âš ï¸  è­¦å‘Š: æ‰¹å¤„ç†å¤§å°è¿‡å°ï¼Œå¯èƒ½å½±å“è®­ç»ƒç¨³å®šæ€§")
        
    # 2. æ£€æŸ¥æ•°æ®
    print("\nğŸ“Š æ•°æ®æ£€æŸ¥:")
    scp_file = config['train_dataset']['args']['scp_file']
    rttm_file = config['train_dataset']['args']['rttm_file']
    
    # ç»Ÿè®¡æ•°æ®é‡
    with open(scp_file) as f:
        num_audio = len(f.readlines())
    print(f"è®­ç»ƒéŸ³é¢‘æ–‡ä»¶æ•°: {num_audio}")
    
    with open(rttm_file) as f:
        num_segments = len(f.readlines())
    print(f"æ ‡æ³¨æ®µæ•°: {num_segments}")
    
    if num_segments < num_audio * 10:
        print("âš ï¸  è­¦å‘Š: æ ‡æ³¨æ®µæ•°ç›¸å¯¹è¾ƒå°‘ï¼Œå¯èƒ½æ•°æ®ä¸è¶³")
    
    # 3. æ£€æŸ¥æ¨¡å‹
    print("\nğŸ—ï¸  æ¨¡å‹æ£€æŸ¥:")
    if checkpoint_file:
        checkpoint = torch.load(checkpoint_file, map_location='cpu')
        
        # å‚æ•°ç»Ÿè®¡
        if 'model_state_dict' in checkpoint:
            model_state = checkpoint['model_state_dict']
            total_params = sum(p.numel() for p in model_state.values())
            print(f"æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")
            
            # æ£€æŸ¥æ¢¯åº¦
            if 'optimizer_state_dict' in checkpoint:
                print("âœ… å‘ç°ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œæ¨¡å‹æ­£åœ¨æ­£å¸¸è®­ç»ƒ")
            else:
                print("âš ï¸  ç¼ºå°‘ä¼˜åŒ–å™¨çŠ¶æ€")
                
        # è®­ç»ƒå†å²
        if 'epoch' in checkpoint:
            print(f"å½“å‰epoch: {checkpoint['epoch']}")
            
        if 'best_score' in checkpoint:
            print(f"æœ€ä½³åˆ†æ•°: {checkpoint['best_score']:.3f}")
    
    # 4. ç³»ç»Ÿèµ„æºæ£€æŸ¥
    print("\nğŸ’» ç³»ç»Ÿæ£€æŸ¥:")
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
        
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    else:
        print("âš ï¸  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆè¾ƒæ…¢ï¼‰")
    
    print("\nâœ… è¯Šæ–­å®Œæˆ")

# ä½¿ç”¨ç¤ºä¾‹
diagnose_training_issues(
    "recipes/diar_ssl/conf/wavlm_updated_conformer.toml",
    "exp/wavlm_updated_conformer/checkpoints/best_model.pt"
)
```

---

## æ€§èƒ½ä¼˜åŒ–

### æ¨ç†ä¼˜åŒ–

#### æ‰¹å¤„ç†ä¼˜åŒ–
```python
class BatchedInference:
    def __init__(self, pipeline, batch_size=16):
        self.pipeline = pipeline
        self.batch_size = batch_size
        
    def process_batch(self, audio_files):
        """æ‰¹é‡å¤„ç†éŸ³é¢‘æ–‡ä»¶"""
        results = {}
        
        # æŒ‰æ‰¹å¤„ç†
        for i in range(0, len(audio_files), self.batch_size):
            batch_files = audio_files[i:i+self.batch_size]
            
            # å¹¶è¡ŒåŠ è½½éŸ³é¢‘
            waveforms = []
            for audio_file in batch_files:
                waveform, sr = torchaudio.load(audio_file)
                waveforms.append(waveform)
            
            # æ‰¹é‡æ¨ç†
            batch_results = self.pipeline.batch_process(waveforms)
            
            # å­˜å‚¨ç»“æœ
            for file, result in zip(batch_files, batch_results):
                results[file] = result
                
        return results
```

#### æ¨¡å‹é‡åŒ–
```python
import torch.quantization as quant

def quantize_model(model, calibration_data):
    """æ¨¡å‹é‡åŒ–ä»¥åŠ é€Ÿæ¨ç†"""
    
    # è®¾ç½®é‡åŒ–é…ç½®
    model.qconfig = quant.get_default_qconfig('fbgemm')
    
    # å‡†å¤‡é‡åŒ–
    quant_model = quant.prepare(model, inplace=False)
    
    # æ ¡å‡†
    quant_model.eval()
    with torch.no_grad():
        for data in calibration_data:
            quant_model(data)
    
    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    quantized_model = quant.convert(quant_model, inplace=False)
    
    return quantized_model

# ä½¿ç”¨ç¤ºä¾‹
# quantized_pipeline = quantize_model(pipeline.model, calibration_data)
```

#### TensorRTä¼˜åŒ–ï¼ˆNVIDIA GPUï¼‰
```python
import tensorrt as trt
import torch_tensorrt

def optimize_with_tensorrt(model, input_shape):
    """ä½¿ç”¨TensorRTä¼˜åŒ–æ¨¡å‹"""
    
    # è½¬æ¢ä¸ºTensorRT
    trt_model = torch_tensorrt.compile(
        model,
        inputs=[torch_tensorrt.Input(input_shape)],
        enabled_precisions={torch.half},  # ä½¿ç”¨FP16
        workspace_size=1 << 20  # 1MB
    )
    
    return trt_model
```

### è®­ç»ƒä¼˜åŒ–

#### æ··åˆç²¾åº¦è®­ç»ƒ
```python
from torch.cuda.amp import GradScaler, autocast

class MixedPrecisionTrainer:
    def __init__(self, model, optimizer):
        self.model = model
        self.optimizer = optimizer
        self.scaler = GradScaler()
        
    def training_step(self, batch):
        self.optimizer.zero_grad()
        
        # è‡ªåŠ¨æ··åˆç²¾åº¦
        with autocast():
            outputs = self.model(batch['waveform'])
            loss = self.compute_loss(outputs, batch['targets'])
        
        # ç¼©æ”¾åå‘ä¼ æ’­
        self.scaler.scale(loss).backward()
        
        # æ›´æ–°å‚æ•°
        self.scaler.step(self.optimizer)
        self.scaler.update()
        
        return loss.item()
```

#### æ•°æ®åŠ è½½ä¼˜åŒ–
```python
from torch.utils.data import DataLoader
import torch.multiprocessing as mp

def create_optimized_dataloader(dataset, batch_size=16):
    """åˆ›å»ºä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨"""
    
    # è®¾ç½®å¤šè¿›ç¨‹å‚æ•°
    num_workers = min(mp.cpu_count(), 8)  # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=True,        # å›ºå®šå†…å­˜ï¼ŒåŠ é€ŸGPUä¼ è¾“
        persistent_workers=True, # ä¿æŒworkerè¿›ç¨‹
        prefetch_factor=2,      # é¢„å–å› å­
        drop_last=True          # ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„batch
    )
    
    return dataloader
```

### å†…å­˜ä¼˜åŒ–

#### æ¢¯åº¦æ£€æŸ¥ç‚¹
```python
import torch.utils.checkpoint as checkpoint

class MemoryEfficientModel(nn.Module):
    def __init__(self, original_model):
        super().__init__()
        self.model = original_model
        
    def forward(self, x):
        # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜å ç”¨
        return checkpoint.checkpoint(self.model, x)
```

#### åŠ¨æ€è°ƒæ•´batch size
```python
class AdaptiveBatchSize:
    def __init__(self, initial_batch_size=16, min_batch_size=1):
        self.current_batch_size = initial_batch_size
        self.min_batch_size = min_batch_size
        
    def adjust_batch_size(self, memory_usage_ratio):
        """æ ¹æ®å†…å­˜ä½¿ç”¨ç‡è°ƒæ•´batch size"""
        
        if memory_usage_ratio > 0.9:  # å†…å­˜ä½¿ç”¨è¶…è¿‡90%
            self.current_batch_size = max(
                self.current_batch_size // 2,
                self.min_batch_size
            )
        elif memory_usage_ratio < 0.6:  # å†…å­˜ä½¿ç”¨ä½äº60%
            self.current_batch_size = min(
                self.current_batch_size * 2,
                64  # æœ€å¤§batch size
            )
            
        return self.current_batch_size
```

---

è¿™ä»½è¯¦å°½çš„æŠ€æœ¯æ–‡æ¡£æ¶µç›–äº†DiariZençš„æ‰€æœ‰é‡è¦æ–¹é¢ï¼Œä»åŸºç¡€ä½¿ç”¨åˆ°é«˜çº§å¼€å‘éƒ½æœ‰è¯¦ç»†è¯´æ˜ã€‚æ–‡æ¡£ç»“æ„æ¸…æ™°ï¼ŒåŒ…å«å¤§é‡ä»£ç ç¤ºä¾‹å’Œå®ç”¨æŠ€å·§ï¼Œç›¸ä¿¡èƒ½å¤Ÿå¸®åŠ©ä½ æ·±å…¥ç†è§£å’Œä½¿ç”¨è¿™ä¸ªå¼ºå¤§çš„è¯´è¯äººåˆ†ç¦»å·¥å…·åŒ…ï¼

---

## é«˜çº§æŠ€æœ¯å®ç°ç»†èŠ‚

### æ„Ÿå—é‡è®¡ç®—è¯¦è§£

DiariZençš„æ„Ÿå—é‡è®¡ç®—å¯¹äºç†è§£æ¨¡å‹æ—¶é—´åˆ†è¾¨ç‡è‡³å…³é‡è¦ï¼š

```python
def compute_receptive_field(model):
    """è®¡ç®—æ¨¡å‹çš„æ„Ÿå—é‡ä¿¡æ¯"""

    # WavLMçš„å·ç§¯å±‚é…ç½®
    wavlm_conv_config = [
        {"kernel": 10, "stride": 5, "padding": 0},  # ç¬¬ä¸€å±‚å·ç§¯
        {"kernel": 3, "stride": 2, "padding": 0},   # ç¬¬äºŒå±‚
        {"kernel": 3, "stride": 2, "padding": 0},   # ç¬¬ä¸‰å±‚
        {"kernel": 3, "stride": 2, "padding": 0},   # ç¬¬å››å±‚
        {"kernel": 3, "stride": 2, "padding": 0},   # ç¬¬äº”å±‚
        {"kernel": 2, "stride": 2, "padding": 0},   # ç¬¬å…­å±‚
        {"kernel": 2, "stride": 2, "padding": 0},   # ç¬¬ä¸ƒå±‚
    ]

    # è®¡ç®—æ„Ÿå—é‡å¤§å°
    receptive_field_size = 1
    for layer in wavlm_conv_config:
        receptive_field_size = (receptive_field_size - 1) * layer["stride"] + layer["kernel"]

    print(f"æ€»æ„Ÿå—é‡å¤§å°: {receptive_field_size} ä¸ªé‡‡æ ·ç‚¹")
    print(f"æ—¶é•¿: {receptive_field_size / 16000:.3f} ç§’")

    # è®¡ç®—æ„Ÿå—é‡ä¸­å¿ƒ
    center = receptive_field_size // 2
    print(f"æ„Ÿå—é‡ä¸­å¿ƒ: {center} ä¸ªé‡‡æ ·ç‚¹")
    print(f"ä¸­å¿ƒæ—¶é•¿: {center / 16000:.3f} ç§’")

    return receptive_field_size, center
```

### å¹‚é›†ç¼–ç ä¼˜åŒ–

**é«˜æ•ˆçš„å¹‚é›†è§£ç å®ç°**ï¼š

```python
class OptimizedPowersetDecoder:
    def __init__(self, max_speakers=4):
        self.max_speakers = max_speakers
        self.num_classes = 2 ** max_speakers

        # é¢„è®¡ç®—æ‰€æœ‰å¯èƒ½çš„è¯´è¯äººç»„åˆ
        self.speaker_combinations = self._generate_combinations()

    def _generate_combinations(self):
        """ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„è¯´è¯äººç»„åˆ"""
        combinations = []
        for class_idx in range(self.num_classes):
            # å°†ç±»åˆ«ç´¢å¼•è½¬æ¢ä¸ºäºŒè¿›åˆ¶å‘é‡
            binary = format(class_idx, f'0{self.max_speakers}b')
            speakers = [i for i, bit in enumerate(binary[::-1]) if bit == '1']
            combinations.append(speakers)
        return combinations

    def decode_batch(self, logits_batch):
        """
        æ‰¹é‡è§£ç å¹‚é›†è¾“å‡ºä¸ºè¯´è¯äººæ´»åŠ¨

        Args:
            logits_batch: [B, T, num_classes] æ‰¹é‡logits
        Returns:
            activities: [B, T, max_speakers] è¯´è¯äººæ´»åŠ¨æ¦‚ç‡
        """

        batch_size, seq_len, num_classes = logits_batch.shape

        # è½¬æ¢ä¸ºæ¦‚ç‡
        probs = torch.softmax(logits_batch, dim=-1)  # [B, T, C]

        # åˆå§‹åŒ–è¯´è¯äººæ´»åŠ¨çŸ©é˜µ
        activities = torch.zeros(batch_size, seq_len, self.max_speakers,
                               dtype=probs.dtype, device=probs.device)

        # å‘é‡åŒ–è§£ç 
        for class_idx, speakers in enumerate(self.speaker_combinations):
            if speakers:  # éç©ºè¯´è¯äººç»„åˆ
                # ä¸ºæ¯ä¸ªæ´»è·ƒè¯´è¯äººç´¯åŠ æ¦‚ç‡
                class_probs = probs[:, :, class_idx].unsqueeze(-1)  # [B, T, 1]
                for speaker_idx in speakers:
                    activities[:, :, speaker_idx] += class_probs.squeeze(-1)

        return activities

    def encode_labels(self, speaker_activities):
        """
        å°†è¯´è¯äººæ´»åŠ¨ç¼–ç ä¸ºå¹‚é›†ç±»åˆ«

        Args:
            speaker_activities: [B, T, max_speakers] è¯´è¯äººæ´»åŠ¨(0/1)
        Returns:
            class_indices: [B, T] ç±»åˆ«ç´¢å¼•
        """

        batch_size, seq_len, max_speakers = speaker_activities.shape

        # è®¡ç®—ç±»åˆ«ç´¢å¼•
        powers = 2 ** torch.arange(max_speakers, device=speaker_activities.device)
        class_indices = torch.sum(speaker_activities * powers, dim=-1)

        return class_indices
```

### å†…å­˜ä¼˜åŒ–æŠ€æœ¯

**æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)**ï¼š

```python
class MemoryEfficientConformer(nn.Module):
    def __init__(self, conformer_config):
        super().__init__()
        self.layers = nn.ModuleList([
            ConformerBlock(**conformer_config)
            for _ in range(conformer_config['num_layers'])
        ])

    def forward(self, x):
        """ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹å‡å°‘å†…å­˜ä½¿ç”¨"""

        def create_custom_forward(module):
            def custom_forward(*inputs):
                return module(*inputs)
            return custom_forward

        # å¯¹æ¯ä¸ªConformerå—åº”ç”¨æ£€æŸ¥ç‚¹
        for layer in self.layers:
            x = torch.utils.checkpoint.checkpoint(
                create_custom_forward(layer),
                x
            )

        return x
```

**è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP)**ï¼š

```python
class MixedPrecisionTrainer:
    def __init__(self, model, optimizer):
        self.model = model
        self.optimizer = optimizer
        self.scaler = torch.cuda.amp.GradScaler()

        # ç¦ç”¨æ¨¡å‹çš„è‡ªåŠ¨æ¢¯åº¦è®¡ç®—ï¼ˆç”±autocastå¤„ç†ï¼‰
        self.model = self.model.to(dtype=torch.float16, memory_format=torch.contiguous_format)

    def training_step(self, batch):
        self.optimizer.zero_grad()

        # è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
        with torch.cuda.amp.autocast():
            outputs = self.model(batch['waveform'])
            loss = self.compute_loss(outputs, batch['labels'])

        # ç¼©æ”¾åå‘ä¼ æ’­
        self.scaler.scale(loss).backward()

        # æ¢¯åº¦è£å‰ªï¼ˆåœ¨scaler.stepä¹‹å‰ï¼‰
        self.scaler.unscale_(self.optimizer)
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

        # æ›´æ–°å‚æ•°
        self.scaler.step(self.optimizer)
        self.scaler.update()

        return loss.item()
```

### åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–

**DeepSpeedé›†æˆ**ï¼š

```python
def setup_deepspeed_training(config_path):
    """é…ç½®DeepSpeedåˆ†å¸ƒå¼è®­ç»ƒ"""

    import deepspeed

    # DeepSpeedé…ç½®
    ds_config = {
        "train_batch_size": 64,
        "gradient_accumulation_steps": 1,
        "optimizer": {
            "type": "AdamW",
            "params": {
                "lr": 1e-3,
                "weight_decay": 0.01
            }
        },
        "scheduler": {
            "type": "WarmupDecayLR",
            "params": {
                "warmup_min_lr": 0,
                "warmup_max_lr": 1e-3,
                "warmup_num_steps": 1000,
                "total_num_steps": 50000
            }
        },
        "fp16": {
            "enabled": True,
            "loss_scale": 0,
            "loss_scale_window": 1000,
            "initial_scale_power": 16,
            "hysteresis": 2,
            "min_loss_scale": 1
        },
        "zero_optimization": {
            "stage": 2,
            "offload_optimizer": {
                "device": "cpu",
                "pin_memory": True
            },
            "allgather_partitions": True,
            "allgather_bucket_size": 2e8,
            "overlap_comm": True,
            "reduce_scatter": True,
            "reduce_bucket_size": 2e8,
            "contiguous_gradients": True
        }
    }

    # åˆå§‹åŒ–DeepSpeed
    model, optimizer, _, lr_scheduler = deepspeed.initialize(
        model=model,
        model_parameters=model.parameters(),
        config=ds_config
    )

    return model, optimizer, lr_scheduler
```

### æ¨¡å‹é‡åŒ–ä¸éƒ¨ç½²

**åŠ¨æ€é‡åŒ–**ï¼š

```python
def quantize_model_for_inference(model_path, quantization_config=None):
    """é‡åŒ–æ¨¡å‹ä»¥åŠ é€Ÿæ¨ç†"""

    # é»˜è®¤é‡åŒ–é…ç½®
    if quantization_config is None:
        quantization_config = torch.quantization.get_default_qconfig('fbgemm')

    # åŠ è½½æ¨¡å‹
    model = load_model(model_path)
    model.eval()

    # å‡†å¤‡é‡åŒ–
    quantized_model = torch.quantization.prepare(model, quantization_config)

    # æ ¡å‡†ï¼ˆä½¿ç”¨å°‘é‡éªŒè¯æ•°æ®ï¼‰
    calibration_data = load_calibration_data()
    with torch.no_grad():
        for batch in calibration_data:
            quantized_model(batch['waveform'])

    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    quantized_model = torch.quantization.convert(quantized_model)

    # ä¿å­˜é‡åŒ–æ¨¡å‹
    torch.save(quantized_model.state_dict(), 'quantized_model.pt')

    return quantized_model
```

**ONNXå¯¼å‡º**ï¼š

```python
def export_to_onnx(model, onnx_path, input_shape=(1, 1, 128000)):
    """å¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼"""

    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    dummy_input = torch.randn(input_shape)

    # è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼
    model.eval()

    # å¯¼å‡ºONNX
    torch.onnx.export(
        model,
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size', 2: 'seq_length'},
            'output': {0: 'batch_size', 1: 'seq_length'}
        }
    )

    print(f"æ¨¡å‹å·²å¯¼å‡ºåˆ°: {onnx_path}")

    # éªŒè¯ONNXæ¨¡å‹
    import onnxruntime as ort
    ort_session = ort.InferenceSession(onnx_path)

    # æ¯”è¾ƒè¾“å‡º
    with torch.no_grad():
        torch_output = model(dummy_input)

    onnx_output = ort_session.run(None, {'input': dummy_input.numpy()})

    # æ£€æŸ¥è¾“å‡ºä¸€è‡´æ€§
    np.testing.assert_allclose(torch_output.numpy(), onnx_output[0], rtol=1e-03, atol=1e-05)
    print("ONNXæ¨¡å‹éªŒè¯é€šè¿‡!")
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•

**å®Œæ•´çš„è¯„ä¼°è„šæœ¬**ï¼š

```python
def comprehensive_benchmark(model, test_dataset, device):
    """å…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    import time
    from torch.profiler import profile, record_function, ProfilerActivity

    model.eval()
    model.to(device)

    # æŒ‡æ ‡æ”¶é›†
    metrics = {
        'latency': [],
        'throughput': [],
        'memory_usage': [],
        'der_scores': []
    }

    # æ€§èƒ½æµ‹è¯•
    with torch.no_grad():
        for i, batch in enumerate(tqdm(test_dataset, desc="Benchmarking")):
            waveform = batch['waveform'].to(device)
            labels = batch['labels']

            # å†…å­˜ç›‘æ§å¼€å§‹
            if device.type == 'cuda':
                torch.cuda.reset_peak_memory_stats()
                torch.cuda.synchronize()

            start_time = time.time()

            # æ¨ç†
            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],
                        record_shapes=True) as prof:
                with record_function("model_inference"):
                    outputs = model(waveform)

            end_time = time.time()

            # è®¡ç®—æŒ‡æ ‡
            latency = end_time - start_time
            throughput = waveform.shape[0] / latency  # æ ·æœ¬/ç§’

            metrics['latency'].append(latency)
            metrics['throughput'].append(throughput)

            # å†…å­˜ä½¿ç”¨
            if device.type == 'cuda':
                peak_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB
                metrics['memory_usage'].append(peak_memory)

            # DERè®¡ç®—
            predictions = torch.sigmoid(outputs)
            der_score = calculate_der_for_batch(predictions, labels)
            metrics['der_scores'].append(der_score)

            if i >= 100:  # åªæµ‹è¯•å‰100ä¸ªæ‰¹æ¬¡
                break

    # è®¡ç®—ç»Ÿè®¡ç»“æœ
    results = {}
    for key, values in metrics.items():
        if values:
            results[key] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'min': np.min(values),
                'max': np.max(values)
            }

    # æ‰“å°ç»“æœ
    print("=== æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ ===")
    print(f"å¹³å‡å»¶è¿Ÿ: {results['latency']['mean']:.3f} Â± {results['latency']['std']:.3f} ç§’")
    print(f"å¹³å‡ååé‡: {results['throughput']['mean']:.1f} Â± {results['throughput']['std']:.1f} æ ·æœ¬/ç§’")
    if 'memory_usage' in results:
        print(f"å³°å€¼å†…å­˜ä½¿ç”¨: {results['memory_usage']['mean']:.1f} Â± {results['memory_usage']['std']:.1f} MB")
    print(f"å¹³å‡DER: {results['der_scores']['mean']:.3f} Â± {results['der_scores']['std']:.3f}")

    return results
```

### æ•…éšœæ’é™¤æŒ‡å—

**å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ**ï¼š

```python
def diagnose_common_issues(error_message, model_config, training_config):
    """è¯Šæ–­å¸¸è§è®­ç»ƒå’Œæ¨ç†é—®é¢˜"""

    diagnoses = []

    # CUDAå†…å­˜ä¸è¶³
    if "CUDA out of memory" in error_message:
        diagnoses.append({
            'issue': 'CUDAå†…å­˜ä¸è¶³',
            'solutions': [
                'å‡å°‘batch_size',
                'å¯ç”¨æ¢¯åº¦ç´¯ç§¯',
                'ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ',
                'å‡å°‘æ¨¡å‹å‚æ•°ï¼ˆå‡å°attention_in, num_layerç­‰ï¼‰',
                'ä½¿ç”¨gradient_checkpointing'
            ]
        })

    # æ¢¯åº¦çˆ†ç‚¸
    if "gradient" in error_message.lower() and ("nan" in error_message or "inf" in error_message):
        diagnoses.append({
            'issue': 'æ¢¯åº¦çˆ†ç‚¸',
            'solutions': [
                'å¯ç”¨æ¢¯åº¦è£å‰ª',
                'é™ä½å­¦ä¹ ç‡',
                'æ£€æŸ¥æ•°æ®è´¨é‡',
                'æ·»åŠ æ¢¯åº¦æ­£åˆ™åŒ–'
            ]
        })

    # æ”¶æ•›é—®é¢˜
    if "loss" in error_message.lower() and "not decreasing" in error_message.lower():
        diagnoses.append({
            'issue': 'è®­ç»ƒä¸æ”¶æ•›',
            'solutions': [
                'æ£€æŸ¥æ•°æ®æ ‡æ³¨è´¨é‡',
                'è°ƒæ•´å­¦ä¹ ç‡è°ƒåº¦',
                'å¢åŠ æ¨¡å‹å®¹é‡',
                'å°è¯•ä¸åŒçš„ä¼˜åŒ–å™¨é…ç½®'
            ]
        })

    # æ¨ç†æ€§èƒ½é—®é¢˜
    if training_config.get('inference_slow', False):
        diagnoses.append({
            'issue': 'æ¨ç†é€Ÿåº¦æ…¢',
            'solutions': [
                'ä½¿ç”¨æ‰¹å¤„ç†æ¨ç†',
                'æ¨¡å‹é‡åŒ–',
                'TensorRTä¼˜åŒ–',
                'å‡å°‘segmentation_stepé‡å '
            ]
        })

    return diagnoses
```

---

è¿™ä»½è¯¦å°½çš„æŠ€æœ¯æ–‡æ¡£ç°åœ¨åŒ…å«äº†DiariZené¡¹ç›®çš„å®Œæ•´æŠ€æœ¯ç»†èŠ‚ï¼Œä»é¡¹ç›®æ¶æ„åˆ°é«˜çº§ä¼˜åŒ–æŠ€æœ¯éƒ½æœ‰è¯¦ç»†è¯´æ˜ã€‚æ–‡æ¡£ä¸ä»…è§£é‡Šäº†"æ˜¯ä»€ä¹ˆ"å’Œ"æ€ä¹ˆç”¨"ï¼Œæ›´é‡è¦çš„æ˜¯è§£é‡Šäº†"ä¸ºä»€ä¹ˆ"å’Œ"èƒŒåçš„åŸç†"ã€‚

å¦‚æœä½ æœ‰ä»»ä½•å…·ä½“é—®é¢˜æˆ–éœ€è¦æ›´æ·±å…¥çš„è§£é‡Šï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ã€‚æˆ‘å¯ä»¥æ ¹æ®ä½ çš„å…·ä½“éœ€æ±‚è¿›ä¸€æ­¥æ‰©å±•æ–‡æ¡£çš„æŸäº›éƒ¨åˆ†ï¼